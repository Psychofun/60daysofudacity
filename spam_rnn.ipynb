{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using an RNN rather than a strictly feedforward network is more accurate since we can include information about the *sequence* of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset: Kaggle SMS Spam Collection\n",
    "[Sms Spam](https://www.kaggle.com/uciml/sms-spam-collection-dataset/downloads/spam.csv/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import  genfromtxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (5575,)\n",
      "v1,v2,,, \n",
      "\n",
      "ham,\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",,, \n",
      "\n",
      "ham,Ok lar... Joking wif u oni...,,, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = genfromtxt('data/spam.csv',delimiter = '\\n', dtype='str')\n",
    "\n",
    "\n",
    "print(\"Shape of data:\", data.shape)\n",
    "print(data[0], \"\\n\")\n",
    "print(data[1],\"\\n\")\n",
    "print(data[2],\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw fist row. \n",
    "data = data[1:]\n",
    "\n",
    "# Separate into messages and labels\n",
    "\n",
    "labels,messages =zip(*list(map( \n",
    "            lambda x: (x[:3]  , x[4:-3]) if x.startswith('h') else (x[:4],x[5:-3])\n",
    "                               \n",
    "                               ,data)))\n",
    "labels = np.array(labels)\n",
    "messages = np.array(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 5574\n",
      "Message 0: \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\" \n",
      " Label: ham \n",
      " \n",
      "Message 1: Ok lar... Joking wif u oni... \n",
      " Label: ham \n",
      " \n",
      "Message 2: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's \n",
      " Label: spam \n",
      " \n",
      "Message 3: U dun say so early hor... U c already then say... \n",
      " Label: ham \n",
      " \n",
      "Message 4: \"Nah I don't think he goes to usf, he lives around here though\" \n",
      " Label: ham \n",
      " \n",
      "Message 5: \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\" \n",
      " Label: spam \n",
      " \n",
      "Message 6: Even my brother is not like to speak with me. They treat me like aids patent. \n",
      " Label: ham \n",
      " \n",
      "Message 7: As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune \n",
      " Label: ham \n",
      " \n",
      "Message 8: WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only. \n",
      " Label: spam \n",
      " \n",
      "Message 9: Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030 \n",
      " Label: spam \n",
      " \n",
      "Message 10: \"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\" \n",
      " Label: ham \n",
      " \n",
      "Message 11: \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\" \n",
      " Label: spam \n",
      " \n",
      "Message 12: \"URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\" \n",
      " Label: spam \n",
      " \n",
      "Message 13: I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times. \n",
      " Label: ham \n",
      " \n",
      "Message 14: I HAVE A DATE ON SUNDAY WITH WILL!! \n",
      " Label: ham \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(len(messages), len(labels))\n",
    "\n",
    "for i in range(15):\n",
    "    print(\"Message {}: {} \\n Label: {} \\n \".format(i,messages[i], labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert labels to float. If spam then 1, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '1' '0' '0' '1' '0' '0' '1' '1']\n",
      "<U1\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "labels[labels == \"spam\"] = 1\n",
    "labels[labels == \"ham\"] = 0\n",
    "print(labels[:10])\n",
    "print(labels[0].dtype)\n",
    "\n",
    "#convert to float\n",
    "labels = labels.astype('float') \n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat \n",
      "\n",
      "ok lar joking wif u oni\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "for k in range(messages.shape[0]):\n",
    "    messages[k] = messages[k].lower()\n",
    "    messages[k] = \"\".join( [s for s in   messages[k] if s not in punctuation])\n",
    "\n",
    "\n",
    "print(messages[0],\"\\n\")\n",
    "print(messages[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go until jurong point crazy available only in bugis n great world la e buffet cine there got amore watok lar joking wif u onifree entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18su dun say so early hor u c already then saynah i dont think he goes to usf he lives around here thoughfreemsg hey there darling its been 3 weeks now and no word back id like some fun you up for it still tb ok xxx std chgs to send å£150 to rcveven my brother is not like to speak with me they treat me like aids patentas per your request melle melle oru minnaminunginte nurungu vettam has been set as your callertune for all callers press 9 to copy your friends callertunewinner as a valued network customer you have been selected to receivea å£900 prize reward to claim call 09061701461 claim code kl341 valid 12 hours onlyhad your mobile 11 months or more u r entitled to update to the latest colour mobiles with camera for f'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_messages=\"\".join( [s.lower() for s in messages if s not in punctuation] )\n",
    "\n",
    "all_messages[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words 78233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'until',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'only',\n",
       " 'in',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'there',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'watok']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = all_messages.split()\n",
    "print(\"Number of words\", len(words))\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'go': 1,\n",
       " 'until': 2,\n",
       " 'point': 3,\n",
       " 'crazy': 4,\n",
       " 'available': 5,\n",
       " 'only': 6,\n",
       " 'in': 7,\n",
       " 'bugis': 8,\n",
       " 'n': 9,\n",
       " 'great': 10,\n",
       " 'world': 11,\n",
       " 'la': 12,\n",
       " 'e': 13,\n",
       " 'cine': 14,\n",
       " 'there': 15,\n",
       " 'got': 16,\n",
       " 'lar': 17,\n",
       " 'wif': 18,\n",
       " 'u': 19,\n",
       " 'entry': 20,\n",
       " '2': 21,\n",
       " 'a': 22,\n",
       " 'wkly': 23,\n",
       " 'comp': 24,\n",
       " 'to': 25,\n",
       " 'win': 26,\n",
       " 'cup': 27,\n",
       " 'final': 28,\n",
       " 'may': 29,\n",
       " 'text': 30,\n",
       " 'receive': 31,\n",
       " 'txt': 32,\n",
       " 'apply': 33,\n",
       " 'dun': 34,\n",
       " 'say': 35,\n",
       " 'so': 36,\n",
       " 'early': 37,\n",
       " 'c': 38,\n",
       " 'already': 39,\n",
       " 'then': 40,\n",
       " 'i': 41,\n",
       " 'dont': 42,\n",
       " 'think': 43,\n",
       " 'he': 44,\n",
       " 'goes': 45,\n",
       " 'usf': 46,\n",
       " 'around': 47,\n",
       " 'here': 48,\n",
       " 'hey': 49,\n",
       " 'darling': 50,\n",
       " 'its': 51,\n",
       " 'been': 52,\n",
       " '3': 53,\n",
       " 'weeks': 54,\n",
       " 'now': 55,\n",
       " 'and': 56,\n",
       " 'no': 57,\n",
       " 'word': 58,\n",
       " 'back': 59,\n",
       " 'id': 60,\n",
       " 'like': 61,\n",
       " 'some': 62,\n",
       " 'fun': 63,\n",
       " 'you': 64,\n",
       " 'up': 65,\n",
       " 'for': 66,\n",
       " 'it': 67,\n",
       " 'still': 68,\n",
       " 'ok': 69,\n",
       " 'xxx': 70,\n",
       " 'std': 71,\n",
       " 'send': 72,\n",
       " 'å£150': 73,\n",
       " 'my': 74,\n",
       " 'brother': 75,\n",
       " 'is': 76,\n",
       " 'not': 77,\n",
       " 'speak': 78,\n",
       " 'with': 79,\n",
       " 'me': 80,\n",
       " 'they': 81,\n",
       " 'treat': 82,\n",
       " 'per': 83,\n",
       " 'your': 84,\n",
       " 'request': 85,\n",
       " 'melle': 86,\n",
       " 'has': 87,\n",
       " 'set': 88,\n",
       " 'as': 89,\n",
       " 'callertune': 90,\n",
       " 'all': 91,\n",
       " 'callers': 92,\n",
       " 'press': 93,\n",
       " '9': 94,\n",
       " 'copy': 95,\n",
       " 'friends': 96,\n",
       " 'valued': 97,\n",
       " 'network': 98,\n",
       " 'customer': 99,\n",
       " 'have': 100,\n",
       " 'selected': 101,\n",
       " 'å£900': 102,\n",
       " 'prize': 103,\n",
       " 'reward': 104,\n",
       " 'claim': 105,\n",
       " 'call': 106,\n",
       " 'code': 107,\n",
       " 'valid': 108,\n",
       " '12': 109,\n",
       " 'hours': 110,\n",
       " 'mobile': 111,\n",
       " '11': 112,\n",
       " 'months': 113,\n",
       " 'or': 114,\n",
       " 'more': 115,\n",
       " 'r': 116,\n",
       " 'entitled': 117,\n",
       " 'update': 118,\n",
       " 'the': 119,\n",
       " 'latest': 120,\n",
       " 'colour': 121,\n",
       " 'mobiles': 122,\n",
       " 'camera': 123,\n",
       " 'free': 124,\n",
       " 'co': 125,\n",
       " 'on': 126,\n",
       " 'gonna': 127,\n",
       " 'be': 128,\n",
       " 'home': 129,\n",
       " 'soon': 130,\n",
       " 'want': 131,\n",
       " 'talk': 132,\n",
       " 'about': 133,\n",
       " 'this': 134,\n",
       " 'stuff': 135,\n",
       " 'tonight': 136,\n",
       " 'k': 137,\n",
       " 'ive': 138,\n",
       " 'enough': 139,\n",
       " 'cash': 140,\n",
       " 'from': 141,\n",
       " '100': 142,\n",
       " 'pounds': 143,\n",
       " 'cost': 144,\n",
       " '16': 145,\n",
       " 'reply': 146,\n",
       " 'hl': 147,\n",
       " '4': 148,\n",
       " 'won': 149,\n",
       " '1': 150,\n",
       " 'week': 151,\n",
       " 'our': 152,\n",
       " 'tc': 153,\n",
       " 'pobox': 154,\n",
       " 'searching': 155,\n",
       " 'right': 156,\n",
       " 'words': 157,\n",
       " 'thank': 158,\n",
       " 'promise': 159,\n",
       " 'wont': 160,\n",
       " 'take': 161,\n",
       " 'help': 162,\n",
       " 'will': 163,\n",
       " 'wonderful': 164,\n",
       " 'at': 165,\n",
       " 'date': 166,\n",
       " 'use': 167,\n",
       " 'credit': 168,\n",
       " 'click': 169,\n",
       " 'wap': 170,\n",
       " 'link': 171,\n",
       " 'next': 172,\n",
       " 'message': 173,\n",
       " 'watching': 174,\n",
       " 'remember': 175,\n",
       " 'how': 176,\n",
       " 'his': 177,\n",
       " 'name': 178,\n",
       " 'yes': 179,\n",
       " 'did': 180,\n",
       " 'v': 181,\n",
       " 'naughty': 182,\n",
       " 'make': 183,\n",
       " 'if': 184,\n",
       " 'way': 185,\n",
       " 'feel': 186,\n",
       " 'miss': 187,\n",
       " 'news': 188,\n",
       " 'ur': 189,\n",
       " 'national': 190,\n",
       " 'team': 191,\n",
       " '87077': 192,\n",
       " 'eg': 193,\n",
       " 'england': 194,\n",
       " 'that': 195,\n",
       " 'seriously': 196,\n",
       " 'going': 197,\n",
       " 'try': 198,\n",
       " 'ha': 199,\n",
       " 'ì': 200,\n",
       " 'pay': 201,\n",
       " 'first': 202,\n",
       " 'when': 203,\n",
       " 'da': 204,\n",
       " 'stock': 205,\n",
       " 'finish': 206,\n",
       " 'lunch': 207,\n",
       " 'down': 208,\n",
       " 'lor': 209,\n",
       " 'ard': 210,\n",
       " 'smth': 211,\n",
       " 'alright': 212,\n",
       " 'can': 213,\n",
       " 'meet': 214,\n",
       " 'myself': 215,\n",
       " 'eat': 216,\n",
       " 'im': 217,\n",
       " 'really': 218,\n",
       " 'hungry': 219,\n",
       " 'tho': 220,\n",
       " 'sucks': 221,\n",
       " 'mark': 222,\n",
       " 'getting': 223,\n",
       " 'worried': 224,\n",
       " 'knows': 225,\n",
       " 'sick': 226,\n",
       " 'pizza': 227,\n",
       " 'always': 228,\n",
       " 'catch': 229,\n",
       " 'bus': 230,\n",
       " 'are': 231,\n",
       " 'an': 232,\n",
       " 'tea': 233,\n",
       " 'eating': 234,\n",
       " 'moms': 235,\n",
       " 'left': 236,\n",
       " 'over': 237,\n",
       " 'dinner': 238,\n",
       " 'do': 239,\n",
       " 'love': 240,\n",
       " 'amp': 241,\n",
       " 'were': 242,\n",
       " 'car': 243,\n",
       " 'ill': 244,\n",
       " 'let': 245,\n",
       " 'know': 246,\n",
       " 'theres': 247,\n",
       " 'work': 248,\n",
       " 'what': 249,\n",
       " 'does': 250,\n",
       " 'thats': 251,\n",
       " 'sure': 252,\n",
       " 'being': 253,\n",
       " 'why': 254,\n",
       " 'x': 255,\n",
       " 'doesnt': 256,\n",
       " 'live': 257,\n",
       " 'was': 258,\n",
       " 'had': 259,\n",
       " 'out': 260,\n",
       " 'she': 261,\n",
       " 'till': 262,\n",
       " 'but': 263,\n",
       " 'we': 264,\n",
       " 'doing': 265,\n",
       " 'too': 266,\n",
       " 'tell': 267,\n",
       " 'anything': 268,\n",
       " 'of': 269,\n",
       " 'just': 270,\n",
       " 'quick': 271,\n",
       " 'ringtone': 272,\n",
       " 'uk': 273,\n",
       " 'charged': 274,\n",
       " 'please': 275,\n",
       " 'confirm': 276,\n",
       " 'by': 277,\n",
       " 'replying': 278,\n",
       " 'look': 279,\n",
       " 'msg': 280,\n",
       " 'again': 281,\n",
       " 'learn': 282,\n",
       " '2nd': 283,\n",
       " 'her': 284,\n",
       " 'lesson': 285,\n",
       " 'see': 286,\n",
       " 'b': 287,\n",
       " 'hows': 288,\n",
       " 'saturday': 289,\n",
       " 'texting': 290,\n",
       " 'youd': 291,\n",
       " 'decided': 292,\n",
       " 'tomo': 293,\n",
       " 'trying': 294,\n",
       " 'wanted': 295,\n",
       " 'weekend': 296,\n",
       " 'forget': 297,\n",
       " 'need': 298,\n",
       " 'crave': 299,\n",
       " 'most': 300,\n",
       " 'sweet': 301,\n",
       " 'tried': 302,\n",
       " 're': 303,\n",
       " 'sms': 304,\n",
       " 'nokia': 305,\n",
       " 'camcorder': 306,\n",
       " '08000930705': 307,\n",
       " 'delivery': 308,\n",
       " 'hope': 309,\n",
       " 'man': 310,\n",
       " 'well': 311,\n",
       " 'am': 312,\n",
       " 'get': 313,\n",
       " 'hopefully': 314,\n",
       " 'cant': 315,\n",
       " 'could': 316,\n",
       " 'maybe': 317,\n",
       " 'ask': 318,\n",
       " 'didnt': 319,\n",
       " 'even': 320,\n",
       " 'hospital': 321,\n",
       " 'kept': 322,\n",
       " 'telling': 323,\n",
       " 'weak': 324,\n",
       " 'time': 325,\n",
       " 'saw': 326,\n",
       " 'usually': 327,\n",
       " 'tomorrow': 328,\n",
       " 'morning': 329,\n",
       " 'hes': 330,\n",
       " 'place': 331,\n",
       " 'never': 332,\n",
       " 'thought': 333,\n",
       " 'liked': 334,\n",
       " 'since': 335,\n",
       " 'best': 336,\n",
       " 'happy': 337,\n",
       " 'sorry': 338,\n",
       " 'give': 339,\n",
       " 'room': 340,\n",
       " 'new': 341,\n",
       " 'red': 342,\n",
       " 'play': 343,\n",
       " 'ice': 344,\n",
       " 'correct': 345,\n",
       " 'end': 346,\n",
       " 'yesterday': 347,\n",
       " 'find': 348,\n",
       " 'year': 349,\n",
       " 'special': 350,\n",
       " 'cinema': 351,\n",
       " 'pass': 352,\n",
       " 'yours': 353,\n",
       " 'etc': 354,\n",
       " 'later': 355,\n",
       " 'where': 356,\n",
       " 'pick': 357,\n",
       " 'yourself': 358,\n",
       " 'move': 359,\n",
       " 'pain': 360,\n",
       " 'good': 361,\n",
       " 'girls': 362,\n",
       " 'situation': 363,\n",
       " 'part': 364,\n",
       " 'checking': 365,\n",
       " 'took': 366,\n",
       " 'forever': 367,\n",
       " 'come': 368,\n",
       " 'double': 369,\n",
       " 'check': 370,\n",
       " 'hair': 371,\n",
       " 'said': 372,\n",
       " 'wun': 373,\n",
       " 'cut': 374,\n",
       " 'pleased': 375,\n",
       " 'following': 376,\n",
       " 'review': 377,\n",
       " 'mob': 378,\n",
       " 'awarded': 379,\n",
       " 'bonus': 380,\n",
       " 'song': 381,\n",
       " 'day': 382,\n",
       " 'which': 383,\n",
       " 'frnds': 384,\n",
       " 'rply': 385,\n",
       " 'complimentary': 386,\n",
       " 'trip': 387,\n",
       " 'å£1000': 388,\n",
       " 'dis': 389,\n",
       " 'hear': 390,\n",
       " 'comes': 391,\n",
       " 'plane': 392,\n",
       " 'month': 393,\n",
       " 'lucky': 394,\n",
       " 'save': 395,\n",
       " 'money': 396,\n",
       " 'class': 397,\n",
       " 'babe': 398,\n",
       " 'wanna': 399,\n",
       " 'something': 400,\n",
       " 'nowi': 401,\n",
       " 'waiting': 402,\n",
       " 'once': 403,\n",
       " 'cool': 404,\n",
       " 'very': 405,\n",
       " 'much': 406,\n",
       " 'after': 407,\n",
       " 'same': 408,\n",
       " 'looking': 409,\n",
       " 'job': 410,\n",
       " 'ah': 411,\n",
       " 'hi': 412,\n",
       " 'urgnt': 413,\n",
       " 'real': 414,\n",
       " 'yo': 415,\n",
       " 'tickets': 416,\n",
       " 'one': 417,\n",
       " 'done': 418,\n",
       " 'used': 419,\n",
       " 'started': 420,\n",
       " 'came': 421,\n",
       " 'bed': 422,\n",
       " 'gotta': 423,\n",
       " 'download': 424,\n",
       " 'wen': 425,\n",
       " 'don‰û÷t': 426,\n",
       " 'stand': 427,\n",
       " 'close': 428,\n",
       " 'another': 429,\n",
       " 'night': 430,\n",
       " 'spent': 431,\n",
       " 'late': 432,\n",
       " 'afternoon': 433,\n",
       " 'means': 434,\n",
       " 'havent': 435,\n",
       " 'any': 436,\n",
       " 'y': 437,\n",
       " 'smile': 438,\n",
       " 'pleasure': 439,\n",
       " 'trouble': 440,\n",
       " 'rain': 441,\n",
       " 'hurts': 442,\n",
       " 'someone': 443,\n",
       " 'loves': 444,\n",
       " 'service': 445,\n",
       " 'representative': 446,\n",
       " '0800': 447,\n",
       " 'between': 448,\n",
       " 'guaranteed': 449,\n",
       " 'å£5000': 450,\n",
       " 'planning': 451,\n",
       " 'buy': 452,\n",
       " '530': 453,\n",
       " 'show': 454,\n",
       " 'simply': 455,\n",
       " 'password': 456,\n",
       " 'po': 457,\n",
       " 'box': 458,\n",
       " 'abt': 459,\n",
       " 'loads': 460,\n",
       " 'wk': 461,\n",
       " 'bit': 462,\n",
       " 'run': 463,\n",
       " 'forgot': 464,\n",
       " 'shower': 465,\n",
       " 'cause': 466,\n",
       " 'prob': 467,\n",
       " 'nothing': 468,\n",
       " 'else': 469,\n",
       " 'price': 470,\n",
       " 'long': 471,\n",
       " 'them': 472,\n",
       " 'ave': 473,\n",
       " 'gone': 474,\n",
       " 'driving': 475,\n",
       " 'test': 476,\n",
       " 'youre': 477,\n",
       " 'mean': 478,\n",
       " 'guess': 479,\n",
       " 'gave': 480,\n",
       " 'men': 481,\n",
       " 'changed': 482,\n",
       " 'search': 483,\n",
       " 'cuz': 484,\n",
       " 'page': 485,\n",
       " 'says': 486,\n",
       " 'life': 487,\n",
       " 'umma': 488,\n",
       " 'lot': 489,\n",
       " 'wishes': 490,\n",
       " 'birthday': 491,\n",
       " 'thanks': 492,\n",
       " 'making': 493,\n",
       " 'hit': 494,\n",
       " 'would': 495,\n",
       " 'address': 496,\n",
       " 'computer': 497,\n",
       " 'isnt': 498,\n",
       " 'old': 499,\n",
       " 'people': 500,\n",
       " 'mom': 501,\n",
       " 'better': 502,\n",
       " 'worry': 503,\n",
       " 'cos': 504,\n",
       " 'things': 505,\n",
       " 'mah': 506,\n",
       " 'contact': 507,\n",
       " 'last': 508,\n",
       " 'weekends': 509,\n",
       " 'draw': 510,\n",
       " 'shows': 511,\n",
       " '12hrs': 512,\n",
       " 'anyway': 513,\n",
       " 'fine': 514,\n",
       " 'juz': 515,\n",
       " 'eatin': 516,\n",
       " 'entered': 517,\n",
       " 'cabin': 518,\n",
       " 'pa': 519,\n",
       " 'bday': 520,\n",
       " 'boss': 521,\n",
       " 'felt': 522,\n",
       " 'askd': 523,\n",
       " 'invited': 524,\n",
       " 'apartment': 525,\n",
       " 'went': 526,\n",
       " 'winner': 527,\n",
       " 'specially': 528,\n",
       " 'holiday': 529,\n",
       " 'flights': 530,\n",
       " 'inc': 531,\n",
       " 'operator': 532,\n",
       " '18': 533,\n",
       " 'must': 534,\n",
       " 'friday': 535,\n",
       " 'uncle': 536,\n",
       " 'paying': 537,\n",
       " 'school': 538,\n",
       " 'directly': 539,\n",
       " 'pls': 540,\n",
       " '2004': 541,\n",
       " 'account': 542,\n",
       " 'statement': 543,\n",
       " 'unredeemed': 544,\n",
       " 'points': 545,\n",
       " 'identifier': 546,\n",
       " 'å£2000': 547,\n",
       " 'caller': 548,\n",
       " 'landline': 549,\n",
       " 'voda': 550,\n",
       " 'numbers': 551,\n",
       " 'ending': 552,\n",
       " 'award': 553,\n",
       " 'match': 554,\n",
       " '08712300220': 555,\n",
       " 'quoting': 556,\n",
       " 'standard': 557,\n",
       " 'rates': 558,\n",
       " 'mu': 559,\n",
       " 'today': 560,\n",
       " 'ìï': 561,\n",
       " 'wat': 562,\n",
       " 'sent': 563,\n",
       " 'yet': 564,\n",
       " 'bother': 565,\n",
       " 'sending': 566,\n",
       " 'girl': 567,\n",
       " 'del': 568,\n",
       " 'bak': 569,\n",
       " 'answer': 570,\n",
       " 'quiz': 571,\n",
       " 'q': 572,\n",
       " 'top': 573,\n",
       " 'sony': 574,\n",
       " 'dvd': 575,\n",
       " 'player': 576,\n",
       " 'country': 577,\n",
       " '82277': 578,\n",
       " 'dogging': 579,\n",
       " 'locations': 580,\n",
       " 'direct': 581,\n",
       " 'join': 582,\n",
       " 'uks': 583,\n",
       " 'bt': 584,\n",
       " 'txting': 585,\n",
       " 'nt': 586,\n",
       " 'haf': 587,\n",
       " 'rooms': 588,\n",
       " 'msgs': 589,\n",
       " 'chat': 590,\n",
       " 'services': 591,\n",
       " 'age': 592,\n",
       " 'yr': 593,\n",
       " 'lazy': 594,\n",
       " 'type': 595,\n",
       " 'lect': 596,\n",
       " 'tired': 597,\n",
       " 'little': 598,\n",
       " 'lovable': 599,\n",
       " 'persons': 600,\n",
       " 'those': 601,\n",
       " 'd': 602,\n",
       " 'their': 603,\n",
       " 'gud': 604,\n",
       " 'open': 605,\n",
       " 'ya': 606,\n",
       " 'who': 607,\n",
       " 'taking': 608,\n",
       " 'replied': 609,\n",
       " 'sexy': 610,\n",
       " 'local': 611,\n",
       " 'luv': 612,\n",
       " 'ltd': 613,\n",
       " 'stop': 614,\n",
       " 'begin': 615,\n",
       " 'pray': 616,\n",
       " 'wine': 617,\n",
       " 'thk': 618,\n",
       " 'sometimes': 619,\n",
       " 'dream': 620,\n",
       " 'without': 621,\n",
       " 'half': 622,\n",
       " 'lots': 623,\n",
       " 'tv': 624,\n",
       " 'become': 625,\n",
       " 'leaving': 626,\n",
       " 'house': 627,\n",
       " 'boy': 628,\n",
       " 'missing': 629,\n",
       " 'years': 630,\n",
       " 'arrange': 631,\n",
       " 'safe': 632,\n",
       " 'because': 633,\n",
       " 'everyone': 634,\n",
       " 'each': 635,\n",
       " 'spend': 636,\n",
       " 'inviting': 637,\n",
       " 'friend': 638,\n",
       " 'frnd': 639,\n",
       " 'order': 640,\n",
       " 'should': 641,\n",
       " 'content': 642,\n",
       " 'goto': 643,\n",
       " 'wit': 644,\n",
       " 'fancy': 645,\n",
       " 'needs': 646,\n",
       " 'completely': 647,\n",
       " 'also': 648,\n",
       " 'bank': 649,\n",
       " 'hop': 650,\n",
       " 'muz': 651,\n",
       " 'discuss': 652,\n",
       " 'liao': 653,\n",
       " 'coming': 654,\n",
       " 'hell': 655,\n",
       " 'believe': 656,\n",
       " 'mr': 657,\n",
       " 'm': 658,\n",
       " 'bath': 659,\n",
       " 'youve': 660,\n",
       " 'carlos': 661,\n",
       " 'staying': 662,\n",
       " 'whole': 663,\n",
       " 'til': 664,\n",
       " 'log': 665,\n",
       " 'spoke': 666,\n",
       " 'wed': 667,\n",
       " 'experience': 668,\n",
       " 'offer': 669,\n",
       " 'especially': 670,\n",
       " 'studying': 671,\n",
       " 'gr8': 672,\n",
       " 'trust': 673,\n",
       " 'guys': 674,\n",
       " 'working': 675,\n",
       " 'towards': 676,\n",
       " 'net': 677,\n",
       " 'wheres': 678,\n",
       " 'boytoy': 679,\n",
       " 'haha': 680,\n",
       " 'awesome': 681,\n",
       " 'freephone': 682,\n",
       " 'xmas': 683,\n",
       " 'times': 684,\n",
       " 'jus': 685,\n",
       " 'reached': 686,\n",
       " 'bathe': 687,\n",
       " 'sis': 688,\n",
       " 'using': 689,\n",
       " 'joined': 690,\n",
       " 'keep': 691,\n",
       " 'touch': 692,\n",
       " 'deal': 693,\n",
       " 'personal': 694,\n",
       " 'finally': 695,\n",
       " 'itself': 696,\n",
       " 'however': 697,\n",
       " 'able': 698,\n",
       " 'every': 699,\n",
       " 'mrng': 700,\n",
       " 'dear': 701,\n",
       " 'hav': 702,\n",
       " 'nice': 703,\n",
       " 'dead': 704,\n",
       " 'tmr': 705,\n",
       " 'orchard': 706,\n",
       " 'mrt': 707,\n",
       " 'kate': 708,\n",
       " 'evening': 709,\n",
       " 'hello': 710,\n",
       " 'darlin': 711,\n",
       " 'finished': 712,\n",
       " 'college': 713,\n",
       " 'ltdecimalgt': 714,\n",
       " 'balance': 715,\n",
       " 'goodmorning': 716,\n",
       " 'sleeping': 717,\n",
       " 'dat': 718,\n",
       " 'oso': 719,\n",
       " 'cannot': 720,\n",
       " 'oredi': 721,\n",
       " 'straight': 722,\n",
       " 'us': 723,\n",
       " 'connection': 724,\n",
       " 'before': 725,\n",
       " 'big': 726,\n",
       " 'break': 727,\n",
       " 'noe': 728,\n",
       " 'slept': 729,\n",
       " 'past': 730,\n",
       " 'few': 731,\n",
       " 'exam': 732,\n",
       " 'march': 733,\n",
       " 'called': 734,\n",
       " 'hard': 735,\n",
       " 'important': 736,\n",
       " 'system': 737,\n",
       " 'leh': 738,\n",
       " 'course': 739,\n",
       " 'happen': 740,\n",
       " 'nite': 741,\n",
       " 'collect': 742,\n",
       " 'appreciate': 743,\n",
       " 'partner': 744,\n",
       " 'off': 745,\n",
       " 'start': 746,\n",
       " 'sign': 747,\n",
       " 'g': 748,\n",
       " 'company': 749,\n",
       " 'than': 750,\n",
       " 'bcoz': 751,\n",
       " 'lessons': 752,\n",
       " 'walk': 753,\n",
       " 'road': 754,\n",
       " 'side': 755,\n",
       " 'battery': 756,\n",
       " 'died': 757,\n",
       " 'yeah': 758,\n",
       " 'flirt': 759,\n",
       " '10p': 760,\n",
       " 'sam': 761,\n",
       " '25': 762,\n",
       " 'oh': 763,\n",
       " 'wil': 764,\n",
       " 'reach': 765,\n",
       " 'argument': 766,\n",
       " 'person': 767,\n",
       " 'kick': 768,\n",
       " 'secret': 769,\n",
       " 'admirer': 770,\n",
       " 'ufind': 771,\n",
       " 'rreveal': 772,\n",
       " 'thinks': 773,\n",
       " 'specialcall': 774,\n",
       " 'laptop': 775,\n",
       " 'case': 776,\n",
       " 'tel': 777,\n",
       " 'meant': 778,\n",
       " 'sounds': 779,\n",
       " 'told': 780,\n",
       " 'into': 781,\n",
       " 'face': 782,\n",
       " 'watch': 783,\n",
       " 'fr': 784,\n",
       " 'thanx': 785,\n",
       " 'everything': 786,\n",
       " 'uve': 787,\n",
       " 'asked': 788,\n",
       " 'him': 789,\n",
       " 'fix': 790,\n",
       " 'ready': 791,\n",
       " 'wake': 792,\n",
       " 'missed': 793,\n",
       " '500': 794,\n",
       " 'cd': 795,\n",
       " 'vouchers': 796,\n",
       " 'music': 797,\n",
       " '87066': 798,\n",
       " 'tncs': 799,\n",
       " 'cal': 800,\n",
       " 'angry': 801,\n",
       " 'wid': 802,\n",
       " 'dnt': 803,\n",
       " 'coz': 804,\n",
       " 'childish': 805,\n",
       " 'true': 806,\n",
       " 'showing': 807,\n",
       " 'deep': 808,\n",
       " 'care': 809,\n",
       " 'takes': 810,\n",
       " 'other': 811,\n",
       " 'gets': 812,\n",
       " 'lemme': 813,\n",
       " 'though': 814,\n",
       " 'lover': 815,\n",
       " 'video': 816,\n",
       " 'handset': 817,\n",
       " '750': 818,\n",
       " 'anytime': 819,\n",
       " 'mins': 820,\n",
       " 'unlimited': 821,\n",
       " 'shopping': 822,\n",
       " 'own': 823,\n",
       " 'disturb': 824,\n",
       " 'ring': 825,\n",
       " 'horny': 826,\n",
       " 'hot': 827,\n",
       " 'unsubscribe': 828,\n",
       " 'dint': 829,\n",
       " 'wana': 830,\n",
       " 'plan': 831,\n",
       " 'hold': 832,\n",
       " 'credits': 833,\n",
       " 'choose': 834,\n",
       " 'club': 835,\n",
       " 'singles': 836,\n",
       " 'quality': 837,\n",
       " 'ended': 838,\n",
       " 'sunny': 839,\n",
       " 'rays': 840,\n",
       " 'leaves': 841,\n",
       " 'worries': 842,\n",
       " 'blue': 843,\n",
       " 'hmv': 844,\n",
       " 'easy': 845,\n",
       " 'questions': 846,\n",
       " '86688': 847,\n",
       " 'might': 848,\n",
       " 'food': 849,\n",
       " 'full': 850,\n",
       " 'swing': 851,\n",
       " 'definitely': 852,\n",
       " 'usual': 853,\n",
       " 'lets': 854,\n",
       " 'baby': 855,\n",
       " 'hour': 856,\n",
       " 'fone': 857,\n",
       " 'calls': 858,\n",
       " 'mail': 859,\n",
       " 'found': 860,\n",
       " 'stupid': 861,\n",
       " 'phone': 862,\n",
       " 'sim': 863,\n",
       " 'card': 864,\n",
       " 'loyalty': 865,\n",
       " 'ends': 866,\n",
       " 'cry': 867,\n",
       " 'die': 868,\n",
       " 'plz': 869,\n",
       " 'rose': 870,\n",
       " 'coffee': 871,\n",
       " 'somebody': 872,\n",
       " 'high': 873,\n",
       " 'imagine': 874,\n",
       " 'second': 875,\n",
       " 'somewhere': 876,\n",
       " '4u': 877,\n",
       " 'book': 878,\n",
       " 'friendship': 879,\n",
       " 'selection': 880,\n",
       " 'games': 881,\n",
       " 'tones': 882,\n",
       " 'babes': 883,\n",
       " 'sport': 884,\n",
       " 'arent': 885,\n",
       " 'accept': 886,\n",
       " 'sister': 887,\n",
       " 'å£200': 888,\n",
       " 'weekly': 889,\n",
       " 'buying': 890,\n",
       " '2day': 891,\n",
       " 'normal': 892,\n",
       " 'rest': 893,\n",
       " 'wot': 894,\n",
       " 'made': 895,\n",
       " 'kb': 896,\n",
       " 'power': 897,\n",
       " 'yoga': 898,\n",
       " 'dunno': 899,\n",
       " 'dude': 900,\n",
       " '11mths': 901,\n",
       " 'christmas': 902,\n",
       " 'ppl': 903,\n",
       " 'pete': 904,\n",
       " 'plans': 905,\n",
       " 'problem': 906,\n",
       " 'todays': 907,\n",
       " 'track': 908,\n",
       " 'reading': 909,\n",
       " 'read': 910,\n",
       " 'return': 911,\n",
       " 'immediately': 912,\n",
       " 'minute': 913,\n",
       " 'fixed': 914,\n",
       " 'line': 915,\n",
       " 'via': 916,\n",
       " 'access': 917,\n",
       " 'number': 918,\n",
       " 'chance': 919,\n",
       " '150pmsg': 920,\n",
       " 'rcvd': 921,\n",
       " 'calling': 922,\n",
       " 'hand': 923,\n",
       " 'post': 924,\n",
       " 'texts': 925,\n",
       " 'wiv': 926,\n",
       " 'interested': 927,\n",
       " 'two': 928,\n",
       " 'round': 929,\n",
       " 'yijue': 930,\n",
       " 'num': 931,\n",
       " 'small': 932,\n",
       " 'txts': 933,\n",
       " '150': 934,\n",
       " 'seeing': 935,\n",
       " 'ever': 936,\n",
       " 'hurt': 937,\n",
       " 'urself': 938,\n",
       " 'fault': 939,\n",
       " 'basically': 940,\n",
       " 'figure': 941,\n",
       " 'jay': 942,\n",
       " 'met': 943,\n",
       " 'insurance': 944,\n",
       " 'ex': 945,\n",
       " 'cashbalance': 946,\n",
       " 'currently': 947,\n",
       " 'maximize': 948,\n",
       " 'cashin': 949,\n",
       " 'cc': 950,\n",
       " 'hgsuite3422lands': 951,\n",
       " 'sleep': 952,\n",
       " 'moment': 953,\n",
       " 'st': 954,\n",
       " 'cold': 955,\n",
       " 'chikku': 956,\n",
       " 'forward': 957,\n",
       " 'meeting': 958,\n",
       " 'air': 959,\n",
       " 'motorola': 960,\n",
       " 'sonyericsson': 961,\n",
       " 'bluetooth': 962,\n",
       " '1000': 963,\n",
       " 'orange': 964,\n",
       " 'mobileupd8': 965,\n",
       " '08000839402': 966,\n",
       " 'discount': 967,\n",
       " 'messages': 968,\n",
       " 'geeee': 969,\n",
       " 'woke': 970,\n",
       " 'wish': 971,\n",
       " 'talking': 972,\n",
       " 'willing': 973,\n",
       " 'reference': 974,\n",
       " 'seen': 975,\n",
       " 'mei': 976,\n",
       " 'didt': 977,\n",
       " 'happening': 978,\n",
       " 'sir': 979,\n",
       " 'project': 980,\n",
       " 'mistake': 981,\n",
       " 'body': 982,\n",
       " 'quite': 983,\n",
       " 'slow': 984,\n",
       " 'guide': 985,\n",
       " 'relax': 986,\n",
       " 'reason': 987,\n",
       " 'couple': 988,\n",
       " 'minutes': 989,\n",
       " 'leave': 990,\n",
       " 'phones': 991,\n",
       " 'rental': 992,\n",
       " 'having': 993,\n",
       " 'sat': 994,\n",
       " 'intro': 995,\n",
       " 'pilates': 996,\n",
       " 'office': 997,\n",
       " 'days': 998,\n",
       " 'bout': 999,\n",
       " 'current': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Build a dictionary that maps words to integers\n",
    "UNK_TOKEN = 'UNK'\n",
    "\n",
    "#Count words\n",
    "word_count ={}\n",
    "for word in words:\n",
    "    r = word_count.get(word,None)\n",
    "    \n",
    "    if r :\n",
    "        word_count[word]+=1\n",
    "    else:\n",
    "        word_count[word] = 1\n",
    "        \n",
    "        \n",
    "\n",
    "#word to index\n",
    "word_to_index = {}\n",
    "\n",
    "keys = word_count.keys()\n",
    "# Begin indexing with 1\n",
    "i= 1\n",
    "for key in  keys:\n",
    "    \n",
    "    if word_count[key] >= 5:\n",
    "        word_to_index[key] = i\n",
    "        i+= 1\n",
    "### Add Unknow token\n",
    "word_to_index[UNK_TOKEN] = 0 \n",
    "        \n",
    "        \n",
    "print(len(word_to_index.keys() ))\n",
    "word_to_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "\n",
    "for key in word_to_index.keys():\n",
    "    \n",
    "    index_to_word[ word_to_index[key] ] = key\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj,path ):\n",
    "    \n",
    "    direct, fname = os.path.split(path)\n",
    "    \n",
    "    if not os.path.exists(direct):\n",
    "        os.makedirs(direct)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(path + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(path):\n",
    "    with open(path  + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index to word \n",
    "save_obj(index_to_word, \"data/index_to_word\")\n",
    "\n",
    "#d = load_obj(\"data/index_to_word\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['go', 'until', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'cine', 'there', 'got', 'lar', 'wif', 'u', 'entry', '2', 'a', 'wkly', 'comp', 'to', 'win', 'cup', 'final', 'may', 'text', 'receive', 'txt', 'apply', 'dun', 'say', 'so', 'early', 'c', 'already', 'then', 'i', 'dont', 'think', 'he', 'goes', 'usf', 'around', 'here', 'hey', 'darling', 'its', 'been', '3', 'weeks', 'now', 'and', 'no', 'word', 'back', 'id', 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still', 'ok', 'xxx', 'std', 'send', 'å£150', 'my', 'brother', 'is', 'not', 'speak', 'with', 'me', 'they', 'treat', 'per', 'your', 'request', 'melle', 'has', 'set', 'as', 'callertune', 'all', 'callers', 'press', '9', 'copy', 'friends', 'valued', 'network', 'customer', 'have', 'selected', 'å£900', 'prize', 'reward', 'claim', 'call', 'code', 'valid', '12', 'hours', 'mobile', '11', 'months', 'or', 'more', 'r', 'entitled', 'update', 'the', 'latest', 'colour', 'mobiles', 'camera', 'free', 'co', 'on', 'gonna', 'be', 'home', 'soon', 'want', 'talk', 'about', 'this', 'stuff', 'tonight', 'k', 'ive', 'enough', 'cash', 'from', '100', 'pounds', 'cost', '16', 'reply', 'hl', '4', 'won', '1', 'week', 'our', 'tc', 'pobox', 'searching', 'right', 'words', 'thank', 'promise', 'wont', 'take', 'help', 'will', 'wonderful', 'at', 'date', 'use', 'credit', 'click', 'wap', 'link', 'next', 'message', 'watching', 'remember', 'how', 'his', 'name', 'yes', 'did', 'v', 'naughty', 'make', 'if', 'way', 'feel', 'miss', 'news', 'ur', 'national', 'team', '87077', 'eg', 'england', 'that', 'seriously', 'going', 'try', 'ha', 'ì', 'pay', 'first', 'when', 'da', 'stock', 'finish', 'lunch', 'down', 'lor', 'ard', 'smth', 'alright', 'can', 'meet', 'myself', 'eat', 'im', 'really', 'hungry', 'tho', 'sucks', 'mark', 'getting', 'worried', 'knows', 'sick', 'pizza', 'always', 'catch', 'bus', 'are', 'an', 'tea', 'eating', 'moms', 'left', 'over', 'dinner', 'do', 'love', 'amp', 'were', 'car', 'ill', 'let', 'know', 'theres', 'work', 'what', 'does', 'thats', 'sure', 'being', 'why', 'x', 'doesnt', 'live', 'was', 'had', 'out', 'she', 'till', 'but', 'we', 'doing', 'too', 'tell', 'anything', 'of', 'just', 'quick', 'ringtone', 'uk', 'charged', 'please', 'confirm', 'by', 'replying', 'look', 'msg', 'again', 'learn', '2nd', 'her', 'lesson', 'see', 'b', 'hows', 'saturday', 'texting', 'youd', 'decided', 'tomo', 'trying', 'wanted', 'weekend', 'forget', 'need', 'crave', 'most', 'sweet', 'tried', 're', 'sms', 'nokia', 'camcorder', '08000930705', 'delivery', 'hope', 'man', 'well', 'am', 'get', 'hopefully', 'cant', 'could', 'maybe', 'ask', 'didnt', 'even', 'hospital', 'kept', 'telling', 'weak', 'time', 'saw', 'usually', 'tomorrow', 'morning', 'hes', 'place', 'never', 'thought', 'liked', 'since', 'best', 'happy', 'sorry', 'give', 'room', 'new', 'red', 'play', 'ice', 'correct', 'end', 'yesterday', 'find', 'year', 'special', 'cinema', 'pass', 'yours', 'etc', 'later', 'where', 'pick', 'yourself', 'move', 'pain', 'good', 'girls', 'situation', 'part', 'checking', 'took', 'forever', 'come', 'double', 'check', 'hair', 'said', 'wun', 'cut', 'pleased', 'following', 'review', 'mob', 'awarded', 'bonus', 'song', 'day', 'which', 'frnds', 'rply', 'complimentary', 'trip', 'å£1000', 'dis', 'hear', 'comes', 'plane', 'month', 'lucky', 'save', 'money', 'class', 'babe', 'wanna', 'something', 'nowi', 'waiting', 'once', 'cool', 'very', 'much', 'after', 'same', 'looking', 'job', 'ah', 'hi', 'urgnt', 'real', 'yo', 'tickets', 'one', 'done', 'used', 'started', 'came', 'bed', 'gotta', 'download', 'wen', 'don‰û÷t', 'stand', 'close', 'another', 'night', 'spent', 'late', 'afternoon', 'means', 'havent', 'any', 'y', 'smile', 'pleasure', 'trouble', 'rain', 'hurts', 'someone', 'loves', 'service', 'representative', '0800', 'between', 'guaranteed', 'å£5000', 'planning', 'buy', '530', 'show', 'simply', 'password', 'po', 'box', 'abt', 'loads', 'wk', 'bit', 'run', 'forgot', 'shower', 'cause', 'prob', 'nothing', 'else', 'price', 'long', 'them', 'ave', 'gone', 'driving', 'test', 'youre', 'mean', 'guess', 'gave', 'men', 'changed', 'search', 'cuz', 'page', 'says', 'life', 'umma', 'lot', 'wishes', 'birthday', 'thanks', 'making', 'hit', 'would', 'address', 'computer', 'isnt', 'old', 'people', 'mom', 'better', 'worry', 'cos', 'things', 'mah', 'contact', 'last', 'weekends', 'draw', 'shows', '12hrs', 'anyway', 'fine', 'juz', 'eatin', 'entered', 'cabin', 'pa', 'bday', 'boss', 'felt', 'askd', 'invited', 'apartment', 'went', 'winner', 'specially', 'holiday', 'flights', 'inc', 'operator', '18', 'must', 'friday', 'uncle', 'paying', 'school', 'directly', 'pls', '2004', 'account', 'statement', 'unredeemed', 'points', 'identifier', 'å£2000', 'caller', 'landline', 'voda', 'numbers', 'ending', 'award', 'match', '08712300220', 'quoting', 'standard', 'rates', 'mu', 'today', 'ìï', 'wat', 'sent', 'yet', 'bother', 'sending', 'girl', 'del', 'bak', 'answer', 'quiz', 'q', 'top', 'sony', 'dvd', 'player', 'country', '82277', 'dogging', 'locations', 'direct', 'join', 'uks', 'bt', 'txting', 'nt', 'haf', 'rooms', 'msgs', 'chat', 'services', 'age', 'yr', 'lazy', 'type', 'lect', 'tired', 'little', 'lovable', 'persons', 'those', 'd', 'their', 'gud', 'open', 'ya', 'who', 'taking', 'replied', 'sexy', 'local', 'luv', 'ltd', 'stop', 'begin', 'pray', 'wine', 'thk', 'sometimes', 'dream', 'without', 'half', 'lots', 'tv', 'become', 'leaving', 'house', 'boy', 'missing', 'years', 'arrange', 'safe', 'because', 'everyone', 'each', 'spend', 'inviting', 'friend', 'frnd', 'order', 'should', 'content', 'goto', 'wit', 'fancy', 'needs', 'completely', 'also', 'bank', 'hop', 'muz', 'discuss', 'liao', 'coming', 'hell', 'believe', 'mr', 'm', 'bath', 'youve', 'carlos', 'staying', 'whole', 'til', 'log', 'spoke', 'wed', 'experience', 'offer', 'especially', 'studying', 'gr8', 'trust', 'guys', 'working', 'towards', 'net', 'wheres', 'boytoy', 'haha', 'awesome', 'freephone', 'xmas', 'times', 'jus', 'reached', 'bathe', 'sis', 'using', 'joined', 'keep', 'touch', 'deal', 'personal', 'finally', 'itself', 'however', 'able', 'every', 'mrng', 'dear', 'hav', 'nice', 'dead', 'tmr', 'orchard', 'mrt', 'kate', 'evening', 'hello', 'darlin', 'finished', 'college', 'ltdecimalgt', 'balance', 'goodmorning', 'sleeping', 'dat', 'oso', 'cannot', 'oredi', 'straight', 'us', 'connection', 'before', 'big', 'break', 'noe', 'slept', 'past', 'few', 'exam', 'march', 'called', 'hard', 'important', 'system', 'leh', 'course', 'happen', 'nite', 'collect', 'appreciate', 'partner', 'off', 'start', 'sign', 'g', 'company', 'than', 'bcoz', 'lessons', 'walk', 'road', 'side', 'battery', 'died', 'yeah', 'flirt', '10p', 'sam', '25', 'oh', 'wil', 'reach', 'argument', 'person', 'kick', 'secret', 'admirer', 'ufind', 'rreveal', 'thinks', 'specialcall', 'laptop', 'case', 'tel', 'meant', 'sounds', 'told', 'into', 'face', 'watch', 'fr', 'thanx', 'everything', 'uve', 'asked', 'him', 'fix', 'ready', 'wake', 'missed', '500', 'cd', 'vouchers', 'music', '87066', 'tncs', 'cal', 'angry', 'wid', 'dnt', 'coz', 'childish', 'true', 'showing', 'deep', 'care', 'takes', 'other', 'gets', 'lemme', 'though', 'lover', 'video', 'handset', '750', 'anytime', 'mins', 'unlimited', 'shopping', 'own', 'disturb', 'ring', 'horny', 'hot', 'unsubscribe', 'dint', 'wana', 'plan', 'hold', 'credits', 'choose', 'club', 'singles', 'quality', 'ended', 'sunny', 'rays', 'leaves', 'worries', 'blue', 'hmv', 'easy', 'questions', '86688', 'might', 'food', 'full', 'swing', 'definitely', 'usual', 'lets', 'baby', 'hour', 'fone', 'calls', 'mail', 'found', 'stupid', 'phone', 'sim', 'card', 'loyalty', 'ends', 'cry', 'die', 'plz', 'rose', 'coffee', 'somebody', 'high', 'imagine', 'second', 'somewhere', '4u', 'book', 'friendship', 'selection', 'games', 'tones', 'babes', 'sport', 'arent', 'accept', 'sister', 'å£200', 'weekly', 'buying', '2day', 'normal', 'rest', 'wot', 'made', 'kb', 'power', 'yoga', 'dunno', 'dude', '11mths', 'christmas', 'ppl', 'pete', 'plans', 'problem', 'todays', 'track', 'reading', 'read', 'return', 'immediately', 'minute', 'fixed', 'line', 'via', 'access', 'number', 'chance', '150pmsg', 'rcvd', 'calling', 'hand', 'post', 'texts', 'wiv', 'interested', 'two', 'round', 'yijue', 'num', 'small', 'txts', '150', 'seeing', 'ever', 'hurt', 'urself', 'fault', 'basically', 'figure', 'jay', 'met', 'insurance', 'ex', 'cashbalance', 'currently', 'maximize', 'cashin', 'cc', 'hgsuite3422lands', 'sleep', 'moment', 'st', 'cold', 'chikku', 'forward', 'meeting', 'air', 'motorola', 'sonyericsson', 'bluetooth', '1000', 'orange', 'mobileupd8', '08000839402', 'discount', 'messages', 'geeee', 'woke', 'wish', 'talking', 'willing', 'reference', 'seen', 'mei', 'didt', 'happening', 'sir', 'project', 'mistake', 'body', 'quite', 'slow', 'guide', 'relax', 'reason', 'couple', 'minutes', 'leave', 'phones', 'rental', 'having', 'sat', 'intro', 'pilates', 'office', 'days', 'bout', 'current', 'actually', 'rock', 'putting', 'put', 'pictures', 'ass', 'god', 'change', 'poly', 'tone', '1st', 'pm', 'stay', 'drink', 'yan', 'jiu', 'den', 'bring', 'dating', 'å£250', 'competition', 'head', 'heart', 'eve', 'yahoo', 'pobox36504w45wq', 'contacted', 'land', '3030', 'voice', 'shell', 'thing', 'mind', 'giving', 'lift', 'wnt', 'vry', 'hv', 'fucking', 'respond', '150ppm', 'ldn', 'ticket', 'story', 'ten', '50', 'tough', 'supposed', 'tot', 'din', 'group', 'doin', 'kinda', 'lol', 'min', 'å£500', 'welcome', 'ure', 'beautiful', 'woman', 'donåõt', 'kind', 'asking', 'bad', 'thru', 'different', 'gives', 'tcs', 'optout', 'princess', 'style', 'enjoy', 'wait', 'many', 'notice', 'tenerife', 'remove', 'flat', 'rate', 'moan', 'cum', 'll', '8th', 'thinking', 'fb', 'activate', 'terms', 'visit', '6', 'shit', 'meh', 'monday', 'either', 'lose', 'some1', 'congratulations', 'bored', 'outside', 'park', 'near', 'rent', 'opinion', '5', 'silent', 'character', '7', '8', 'stylish', 'simple', '40gb', 'ipod', 'mp3', '150p', 'pin', 'less', 'fri', 'children', 'attempt', 'merry', 'worth', '85023', 'savamob', 'member', 'offers', 't', 'cs', 'å£300', 'sub', 'unsub', 'pretty', 'within', '2003', '800', 'expires', 'while', 'hoping', 'malaria', 'kiss', 'across', 'fat', 'fingers', 'these', 'returns', 'deari', 'age16', 'quote', 'listen', 'self', 'mine', 'rather', 'hotel', 'english', 'omw', 'through', 'water', 'warm', 'youll', 'weight', 'cheap', 'pics', 'fast', 'workin', 'huh', 'barely', 'fuck', '2mrw', 'gym', 'whatever', 'daddy', 'scream', 'mum', 'sch', 'clean', 'kids', 'jazz', 'yogasana', 'em', 'spree', 'shop', 'tscs', 'custcare', 'pound', 's', 'announcement', 'train', 'present', 'stopped', 'info', 'imma', 'euro2004', 'results', 'daily', 'valentine', 'game', 'valentines', 'almost', 'family', 'aft', 'snow', 'weather', 'brings', 'together', 'alex', 'far', 'pub', 'drinks', 'paid', 'longer', 'darren', 'area', '25p', 'vodafone', 'å£350', 'matches', 'forwarded', '‰ûò', 'voucher', 'holder', 'pc', 'login', 'dad', 'question', 'sound', 'probably', 'cartoon', 'listening', 'happened', 'gentle', 'starting', 'town', 'hella', 'drug', 'earth', 'sense', 'unless', 'sea', 'envelope', 'paper', 'fetch', 'de', 'study', 'ho', 'belly', 'laugh', '20', 'opt', 'saying', 'knw', 'awake', 'xy', 'sex', 'bedroom', 'king', 'shall', 'ans', 'torch', 'urgent', 'sun', 'law', 'looks', 'gap', '‰û', 'idea', 'away', 'fantastic', 'wer', 'sit', 'aftr', 'dey', 'seems', 'smoke', 'details', 'sell', 'b4', 'both', 'running', 'feeling', 'wants', 'email', 'tonite', 'add', 'å£10', 'mayb', 'rite', 'gd', 'glad', '10', 'anyone', 'makes', 'yest', 'summer', 'store', 'goin', 'wonder', 'lateri', 'theyre', 'drive', 'damn', 'wats', 'picking', 'lovely', 'guy', 'wishing', 'happiness', 'slave', 'turns', 'couldnt', 'shes', 'polyphonic', 'lost', 'pobox84', 'no1', 'mates', 'wwwgetzedcouk', '36504', 'w45wq', 'aint', 'gift', 'future', 'å£100', 'bag', 'bid', 'ones', 'model', 'youi', 'gettin', '2u', 'teasing', 'wwwcomuknet', 'extra', 'charge', 'charity', '8007', 'polys', 'zed', 'light', 'mate', 'five', 'understand', 'mother', 'await', 'collection', 'sae', 'wan', 'version', 'poor', 'sn', 'fall', 'whenever', 'sort', 'birds', 'user', 'asap', 'drop', 'otherwise', 'ntt', 'knew', 'movie', 'busy', 'booked', 'due', 'teach', 'whats', 'plus', 'sale', 'gay', '08712460324', 'ym', 'sad', 'pissed', 'calli', 'iam', 'wrong', 'photo', 'registered', 'mo', 'wherever', 'wear', 'å£10000', 'least', 'places', 'medical', 'recently', 'expensive', 'photos', '08718720201', 'truth', 'against', 'feels', 'heavy', 'brand', 'decide', 'å£800', 'site', 'wife', 'earlier', 'nxt', 'il', 'weed', '3g', 'videophones', 'videochat', 'java', 'dload', 'noline', 'loving', 'party', 'miracle', 'comin', 'heard', 'contract', 'frm', 'j', 'asleep', 'eh', 'yup', 'okay', 'loverboy', 'during', 'starts', 'april', 'flower', 'f', 'support', 'revealed', 'xchat', 'na', 'movies', 'regards', 'pic', 'fight', 'sipix', 'digital', 'p', 'receipt', 'information', 'o2', 'onto', 'httpwwwurawinnercom', 'surprise', 'awaiting', 'online', 'italian', 'gal', 'whos', 'film', 'ago', 'luck', 'raining', 'station', 'cute', 'drugs', 'energy', 'nw', 'chennai', 'choice', 'enter', 'mite', 'complete', '150ptone', 'parents', 'picked', '10pmin', 'hee', 'cell', 'mode', 'depends', 'wondering', 'others', 'dog', 'tour', 'dreams', 'broke', 'original', 'bb', 'shd', 'wow', 'vl', 'frens', 'super', 'convey', 'tht', 'nigeria', 'reaching', 'fantasies', '08707509020', '20p', '1327', 'croydon', 'cr9', '5wb', 'write', 'fact', 'linerental', 'walking', 'meaning', 'girlfrnd', 'o', 'players', 'cafe', 'rem', 'black', 'hurry', 'nobody', 'alone', '2nite', '4th', 'nature', 'w', 'keeping', 'google', 'smiling', 'vomit', '0870', 'loved', 'except', 'internet', 'arrive', 'screaming', 'airport', 'auction', '86021', 'difficult', 'south', 'tampa', 'costs', 'gas', 'john', 'bill', 'surely', 'gods', 'instead', '250', 'london', 'buzz', 'grins', 'wasnt', 'lei', 'neva', 'waking', 'lookin', 'inside', 'sight', 'bold', 'don', 'congrats', 'ran', 'under', 'boys', 'hook', 'bin', 'slowly', 'father', 'joys', 'selling', 'buns', 'space', 'arcade', 'created', 'holding', 'eyes', 'exciting', 'hate', 'wednesday', 'thnk', 'excuse', 'okie', 'costa', 'sol', '09050090044', 'toclaim', 'pobox334', 'stockport', 'sk38xh', 'costå£150pm', 'moral', 'thinkin', 'mon', 'theatre', 'obviously', 'boost', 'sitting', 'ahmad', 'training', 'ge', 'official', 'flag', 'inclusive', 'ip4', 'looked', 'expecting', 'pix', 'colleagues', 'sed', 'mood', 'minuts', 'latr', 'caken', 'sofa', 'funny', 'empty', 'role', 'checked', 'unable', 'tear', 'street', 'worse', 'whether', 'ringtones', 'sky', 'hw', 'izzit', 'marry', 'tuesday', 'murdered', 'india', 'waste', 'happens', 'deliver', 'behind', 'amazing', 'hr', 'cancer', 'tariffs', '28', 'cover', 'none', 'cancel', 'planned', 'wouldnt', 'relation', 'share', 'lik', 'asks', '3510i', '300', 'meds', 'oki', 'single', 'mths', 'nowhi', 'å£400', 'common', 'works', 'UNK'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save word to index \n",
    "save_obj(word_to_index, \"data/word_to_index\")\n",
    "#e = load_obj(\"data/word_to_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word and get index fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(index_to_word, index):\n",
    "    \"\"\"\n",
    "    index_to_word: dictionary\n",
    "        index to word dict\n",
    "    index: int\n",
    "    \n",
    "    return word given index. If index (key) not in dict returns 'UNK' unknow token\n",
    "    \"\"\"\n",
    "    \n",
    "    result = index_to_word.get(index,None)\n",
    "    \n",
    "    if result:\n",
    "        return result\n",
    "    return UNK_TOKEN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word_to_index, word):\n",
    "    \"\"\"\n",
    "    word_to_index: dictionary\n",
    "        word to index dict\n",
    "    word: string\n",
    "    return index of the word from word_to_index\n",
    "    if word not in word_to_index return 0, index of unknow token.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    result = word_to_index.get(word,None)\n",
    "    \n",
    "    if result: \n",
    "        return result\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la \n",
      " 412\n"
     ]
    }
   ],
   "source": [
    "print(get_word(index_to_word, 12),\"\\n\",get_index(word_to_index,\"hi\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 14, 15, 16, 0, 562] \n",
      "\n",
      "[69, 17, 0, 18, 19, 0] \n",
      "\n",
      "[124, 20, 7, 21, 22, 23, 24, 25, 26, 0, 27, 28, 0, 0, 29, 0, 30, 0, 25, 0, 25, 31, 20, 0, 32, 0, 33, 0] \n",
      "\n",
      "[19, 34, 35, 36, 37, 0, 19, 38, 39, 40, 35] \n",
      "\n",
      "[0, 41, 42, 43, 44, 45, 25, 46, 44, 0, 47, 48, 814] \n",
      "\n",
      "[0, 49, 15, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 0, 69, 70, 71, 0, 25, 72, 73, 25, 0] \n",
      "\n",
      "[320, 74, 75, 76, 77, 61, 25, 78, 79, 80, 81, 82, 80, 61, 0, 0] \n",
      "\n",
      "[89, 83, 84, 85, 86, 86, 0, 0, 0, 0, 87, 52, 88, 89, 84, 90, 66, 91, 92, 93, 94, 25, 95, 84, 96, 90] \n",
      "\n",
      "[527, 89, 22, 97, 98, 99, 64, 100, 52, 101, 25, 0, 102, 103, 104, 25, 105, 106, 0, 105, 107, 0, 108, 109, 110, 6] \n",
      "\n",
      "[259, 84, 111, 112, 113, 114, 115, 19, 116, 117, 25, 118, 25, 119, 120, 121, 122, 79, 123, 66, 124, 106, 119, 111, 118, 125, 124, 126, 0] \n",
      "\n",
      "Zero length messages\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "\n",
    "for message in messages:\n",
    "    \n",
    "    vector = [ get_index(word_to_index,w) for w in message.split()]\n",
    "    vectors.extend([vector])\n",
    "\n",
    "\n",
    "for  j in range(10):\n",
    "    print(vectors[j],\"\\n\")\n",
    "    \n",
    "print(\"Zero length messages\")    \n",
    "for  j in range(len(vectors)):\n",
    "    if   len(vectors[j]) == 0:\n",
    "        \n",
    "        #print(vectors[j],\"\\n\")\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Getting rid of extremely long or short messages; the outliers\n",
    "2. Padding/truncating the remaining data so that we have messages of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length messages: 12\n",
      "Maximum message length: 171\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#outliers messages stats\n",
    "messages_lens = Counter([len(x)  for x in vectors])\n",
    "print(\"Zero-length messages: {}\".format(messages_lens[0]))\n",
    "print(\"Maximum message length: {}\".format(max(messages_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages before removing outliers:  5574\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of messages before removing outliers: \", len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages after removing outliers:  5562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "non_zero_idx = [i for i,message in enumerate(vectors) if len(message)!= 0 ]\n",
    "\n",
    "#remove 0 length messages end their labels \n",
    "vectors = [vectors[i] for i in non_zero_idx ]\n",
    "labels = np.array([labels[i] for i in non_zero_idx])\n",
    "\n",
    "print(\"Number of messages after removing outliers: \",len(vectors))\n",
    "\n",
    "\n",
    "\n",
    "len(vectors) == labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(vectors, seq_length):\n",
    "    ''' Return features of vectors, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(vectors), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(vectors):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  16   0 562]\n",
      " [  0   0   0 ...  18  19   0]\n",
      " [  0   0   0 ...   0  33   0]\n",
      " ...\n",
      " [  0   0   0 ...  84  96  90]\n",
      " [  0   0   0 ... 109 110   6]\n",
      " [  0   0   0 ... 124 126   0]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(vectors, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(vectors), \"Your features should have as many rows as vectors.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "\n",
    "\n",
    "# print first 200 values of the first 10 batches \n",
    "print(features[:10,:200])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(4449, 200) \n",
      "Validation set: \t(556, 200) \n",
      "Test set: \t\t(557, 200)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Create Tensor datasets\n",
    "# CONVERT TO int64 for embedding layer.\n",
    "train_data = TensorDataset(th.from_numpy(train_x).to(th.int64), th.from_numpy(train_y))\n",
    "valid_data = TensorDataset(th.from_numpy(val_x).to(th.int64)  , th.from_numpy(val_y))\n",
    "test_data = TensorDataset(th.from_numpy(test_x).to(th.int64)  , th.from_numpy(test_y))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([32, 200])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ...,    7,  414,  487],\n",
      "        [   0,    0,    0,  ..., 1578,    9,    0],\n",
      "        [   0,    0,    0,  ...,   77,  390,  925],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,   25,  267,    0],\n",
      "        [   0,    0,    0,  ...,  255,    0,    0],\n",
      "        [   0,    0,    0,  ...,  128,  232,    0]]) torch.int64\n",
      "\n",
      "Sample label size:  torch.Size([32])\n",
      "Sample label: \n",
      " tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x, sample_x.dtype)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=th.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SpamRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Spam Detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SpamRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob,\n",
    "                            batch_first=True\n",
    "                           )\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        train_on_gpu=th.cuda.is_available()\n",
    "        #print(\"Train on GPU:\", train_on_gpu)\n",
    "\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3\n",
    "\n",
    "> **Important:** Define the model  hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpamRNN(\n",
      "  (embedding): Embedding(1645, 200)\n",
      "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(index_to_word) \n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "net = SpamRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset.\n",
    "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 0.048679... Val Loss: 0.087054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Instalaciones\\Anaconda3\\envs\\privateai\\lib\\site-packages\\torch\\nn\\modules\\loss.py:512: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3... Step: 200... Loss: 0.083733... Val Loss: 0.075113\n",
      "Epoch: 3/3... Step: 300... Loss: 0.177228... Val Loss: 0.072674\n",
      "Epoch: 3/3... Step: 400... Loss: 0.002527... Val Loss: 0.074421\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net = net.cuda()\n",
    "    \n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "        # initialize hidden state\n",
    "        # Variable batch size for len of dataset  not divisible by batch_size\n",
    "        current_batch_size = inputs.shape[0] \n",
    "        h = net.init_hidden(current_batch_size)\n",
    "        \n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        #print(\"h shape\", h[0].shape)\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            #val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                # initialize hidden state\n",
    "                # Variable batch size for len of  dataset  not divisible by batch_size\n",
    "                current_batch_size = inputs.shape[0] \n",
    "                val_h = net.init_hidden(current_batch_size)\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                #print(\"inputs shape\", inputs.shape)\n",
    "                #print(\"val_h shape\", val_h[0].shape, val_h[1].shape)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "There are a few ways to test your network.\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
    "\n",
    "* **Inference on user-generated data:** Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called **inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.072\n",
      "Test accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    \n",
    "    # initialize hidden state\n",
    "    # Variable batch size for len of dataset  not divisible by batch_size\n",
    "    current_batch_size = inputs.shape[0] \n",
    "    h = net.init_hidden(current_batch_size)\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = th.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_spam = \"Hi, will go to the beach this week. You are invited\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on a new message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[412, 163, 1, 25, 119, 0, 134, 151, 64, 231, 524]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_message(message):\n",
    "    message = message.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in message if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([get_index(word_to_index,word) for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_message(no_spam)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0 412 163   1  25 119   0 134 151  64\n",
      "  231 524]]\n"
     ]
    }
   ],
   "source": [
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = th.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, message, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_message(message)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = th.from_numpy(features)\n",
    "    # cast tensor to int64\n",
    "    feature_tensor = feature_tensor.to(th.int64)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = th.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"SPAM detected!\")\n",
    "    else:\n",
    "        print(\"No spam message\")\n",
    "        \n",
    "    return 1 if pred.item()==1 else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam message\n",
    "spam = 'URGENT! claim your prize. Be a winner, new lot ends at nigth. Like and suscribe '\n",
    "\n",
    "no_spam = 'Hi, will go to the beach this week. You are invited'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.990310\n",
      "SPAM detected!\n",
      "Prediction value, pre-rounding: 0.006003\n",
      "No spam message\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, spam, seq_length)\n",
    "predict(net, no_spam, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "import os \n",
    "\n",
    "model_path = \"./models/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "file_path = model_path + \"rnn\"\n",
    "    \n",
    "th.save(net.state_dict(),file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpamRNN(\n",
       "  (embedding): Embedding(1645, 200)\n",
       "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "# Model class must be defined somewhere\n",
    "model = SpamRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.load_state_dict(th.load(file_path))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load dictionaries \n",
    "index_to_word= load_obj(\"data/index_to_word\")\n",
    "word_to_index = load_obj(\"data/word_to_index\")\n",
    "\n",
    "def classify(model,message= \"\",sequence_length = 200):\n",
    "    \n",
    "    vector = tokenize_message(message)\n",
    "    vector = pad_features(vector , sequence_length)\n",
    "    \n",
    "    class_ = predict(net,message,sequence_length)\n",
    "    \n",
    "    return class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.037792\n",
      "No spam message\n"
     ]
    }
   ],
   "source": [
    "classify(model, \"New offers come to nigth. 30% or until 90% off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privateai",
   "language": "python",
   "name": "privateai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
