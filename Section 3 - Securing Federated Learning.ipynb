{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Securing Federated Learning\n",
    "\n",
    "- Lesson 1: Trusted Aggregator\n",
    "- Lesson 2: Intro to Additive Secret Sharing\n",
    "- Lesson 3: Intro to Fixed Precision Encoding\n",
    "- Lesson 4: Secret Sharing + Fixed Precision in PySyft\n",
    "- Final Project: Federated Learning wtih Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Federated Learning with a Trusted Aggregator\n",
    "\n",
    "In the last section, we learned how to train a model on a distributed dataset using Federated Learning. In particular, the last project aggregated gradients directly from one data owner to another. \n",
    "\n",
    "However, while in some cases it could be ideal to do this, what would be even better is to be able to choose a neutral third party to perform the aggregation.\n",
    "\n",
    "As it turns out, we can use the same tools we used previously to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Federated Learning with a Trusted Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this project here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch as th\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 12:47:01.570128 13956 hook.py:97] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = th.tensor([[1.0,1.0],[3,1.0],[1,1.0],[0,1.0],[23.0,1.0],[6,1.0],[7,1.0],[15,1.0],[9.0,1.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "target = th.tensor([[2.],[6.0], [2.0], [0],[46.0],[12.0],[14.0],[30.0],[18.0]], requires_grad=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3 #number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers  =[sy.VirtualWorker(hook, id = \"w\"+ str(i)) for i in range(m)]\n",
    "chunk_size = data.shape[0]//m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets[0]\n",
      "tensor([[1., 1.],\n",
      "        [3., 1.],\n",
      "        [1., 1.]], grad_fn=<SliceBackward>)\n",
      "tensor([[2.],\n",
      "        [6.],\n",
      "        [2.]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "#make a mini-dataset, one per worker\n",
    "datasets = [ [\n",
    "              data[j*chunk_size : (j+1) * chunk_size  ] ,\n",
    "              target[j*chunk_size : (j+1) * chunk_size] \n",
    "             ]\n",
    "            \n",
    "             for  j in range(m)\n",
    "           ]\n",
    "\n",
    "print(\"Datasets[0]\", *datasets[0], sep= \"\\n\")\n",
    "\n",
    "#send datasets to workers\n",
    "for k in range(m):\n",
    "    \n",
    "    datasets[k][0] = datasets[k][0].send(workers[k])\n",
    "    \n",
    "    datasets[k][1] = datasets[k][1].send(workers[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def train_secure_grads(m,datasets,workers, iterations=20):\n",
    "    \"\"\"\n",
    "    iterations: int\n",
    "    \n",
    "    m: int\n",
    "        number of models/workers\n",
    "        \n",
    "    datasets: list of lists of tensors\n",
    "        [ [data_1,targets_1],[data_2, targets_2], ...     ]\n",
    "    \n",
    "    workers: list of VirtualWorkers\n",
    "    \n",
    "    iterations: int \n",
    "        number of SGD steps to do.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a model for each worker \n",
    "    models = [ nn.Linear(2,1) for _ in range(m)]\n",
    "    \n",
    "    \n",
    "    # Global model\n",
    "    global_model = nn.Linear(2,1)\n",
    "    \n",
    "    # Create optims \n",
    "    optims = []\n",
    "\n",
    "    for i,t in enumerate(datasets):\n",
    "        \n",
    "        _data,_target  = t[0],t[1]\n",
    "        \n",
    "        \n",
    "        # Data preparation\n",
    "        \n",
    "        # send model to the data\n",
    "        models[i] = models[i].send(_data.location)\n",
    "        \n",
    "        #create optimizer for model i \n",
    "        #learning rate >= 0.1, makess trainig diverge on most models with little data.\n",
    "        optim_i = optim.SGD(params=models[i].parameters(), lr=0.001)\n",
    "        optims.extend( [optim_i] )\n",
    "        \n",
    "        for _ in range(iterations):            \n",
    "            # do normal training\n",
    "            optims[i].zero_grad()\n",
    "            pred = models[i](_data)\n",
    "            loss = ((pred - _target)**2).sum()\n",
    "            loss.backward()\n",
    "            \n",
    "            optims[i].step()\n",
    "\n",
    "            \n",
    "            print(\"Loss: {} for model: {}\".format( loss.clone().get().item(), i ))\n",
    "        \n",
    "        print(\"Params {} for model: {}\".format(models[i].weight.clone().get() ,i))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Average models on selected worker\n",
    "    \n",
    "    #shuffle models. Select first to aggregate gradients\n",
    "    shuffle(models)\n",
    "    \n",
    "    #Trusted agregator\n",
    "    # Selected worker to aggregate gradients.\n",
    "    sel_model = models[0]\n",
    "    \n",
    "    #move_model = lambda x: x.move(sel_worker)\n",
    "\n",
    "    w,b = sel_model.weight.data ,sel_model.bias.data\n",
    "    \n",
    "    for model in models[1:]:\n",
    "        # Move model to selected worker\n",
    "        model.move(sel_model.location)\n",
    "        \n",
    "        w+= model.weight.data\n",
    "        b+=  model.bias.data\n",
    "    \n",
    "    w = w/m\n",
    "    b = b/m\n",
    "    \n",
    "    with th.no_grad():\n",
    "    \n",
    "        global_model.weight.set_(  w.get() \n",
    "                                  )\n",
    "        global_model.bias.set_(    b.get() \n",
    "                                  )\n",
    "    \n",
    "    print(\"Params for model global model: {}\".format(global_model.weight.clone()))\n",
    "\n",
    "        \n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 58.597496032714844 for model: 0\n",
      "Loss: 55.003089904785156 for model: 0\n",
      "Loss: 51.63465881347656 for model: 0\n",
      "Loss: 48.47798156738281 for model: 0\n",
      "Loss: 45.51971435546875 for model: 0\n",
      "Loss: 42.747371673583984 for model: 0\n",
      "Loss: 40.14923858642578 for model: 0\n",
      "Loss: 37.71435546875 for model: 0\n",
      "Loss: 35.432437896728516 for model: 0\n",
      "Loss: 33.293853759765625 for model: 0\n",
      "Loss: 31.28957748413086 for model: 0\n",
      "Loss: 29.411151885986328 for model: 0\n",
      "Loss: 27.650657653808594 for model: 0\n",
      "Loss: 26.00066375732422 for model: 0\n",
      "Loss: 24.454212188720703 for model: 0\n",
      "Loss: 23.004791259765625 for model: 0\n",
      "Loss: 21.646284103393555 for model: 0\n",
      "Loss: 20.37297248840332 for model: 0\n",
      "Loss: 19.179485321044922 for model: 0\n",
      "Loss: 18.06080436706543 for model: 0\n",
      "Loss: 17.01221466064453 for model: 0\n",
      "Loss: 16.029308319091797 for model: 0\n",
      "Loss: 15.107946395874023 for model: 0\n",
      "Loss: 14.244258880615234 for model: 0\n",
      "Loss: 13.434610366821289 for model: 0\n",
      "Loss: 12.675599098205566 for model: 0\n",
      "Loss: 11.964044570922852 for model: 0\n",
      "Loss: 11.296950340270996 for model: 0\n",
      "Loss: 10.671524047851562 for model: 0\n",
      "Loss: 10.085139274597168 for model: 0\n",
      "Loss: 9.53533935546875 for model: 0\n",
      "Loss: 9.01982593536377 for model: 0\n",
      "Loss: 8.5364351272583 for model: 0\n",
      "Loss: 8.083149909973145 for model: 0\n",
      "Loss: 7.658074378967285 for model: 0\n",
      "Loss: 7.259433269500732 for model: 0\n",
      "Loss: 6.885561466217041 for model: 0\n",
      "Loss: 6.5349016189575195 for model: 0\n",
      "Loss: 6.20599365234375 for model: 0\n",
      "Loss: 5.897468090057373 for model: 0\n",
      "Loss: 5.608043193817139 for model: 0\n",
      "Loss: 5.336518287658691 for model: 0\n",
      "Loss: 5.081767559051514 for model: 0\n",
      "Loss: 4.842733860015869 for model: 0\n",
      "Loss: 4.618428707122803 for model: 0\n",
      "Loss: 4.407927989959717 for model: 0\n",
      "Loss: 4.210363388061523 for model: 0\n",
      "Loss: 4.024921894073486 for model: 0\n",
      "Loss: 3.850839614868164 for model: 0\n",
      "Loss: 3.6874020099639893 for model: 0\n",
      "Params tensor([[1.0311, 1.1863]], requires_grad=True) for model: 0\n",
      "\n",
      "Loss: 3927.342529296875 for model: 1\n",
      "Loss: 72.6841812133789 for model: 1\n",
      "Loss: 1.4025458097457886 for model: 1\n",
      "Loss: 0.08370489627122879 for model: 1\n",
      "Loss: 0.058626286685466766 for model: 1\n",
      "Loss: 0.05747741460800171 for model: 1\n",
      "Loss: 0.05678042396903038 for model: 1\n",
      "Loss: 0.05609970539808273 for model: 1\n",
      "Loss: 0.05542623624205589 for model: 1\n",
      "Loss: 0.054761841893196106 for model: 1\n",
      "Loss: 0.05410455912351608 for model: 1\n",
      "Loss: 0.05345689505338669 for model: 1\n",
      "Loss: 0.052815575152635574 for model: 1\n",
      "Loss: 0.052181676030159 for model: 1\n",
      "Loss: 0.05155659466981888 for model: 1\n",
      "Loss: 0.05093766003847122 for model: 1\n",
      "Loss: 0.050326935946941376 for model: 1\n",
      "Loss: 0.04972367733716965 for model: 1\n",
      "Loss: 0.04912741482257843 for model: 1\n",
      "Loss: 0.04853826016187668 for model: 1\n",
      "Loss: 0.04795616865158081 for model: 1\n",
      "Loss: 0.04738058149814606 for model: 1\n",
      "Loss: 0.04681265354156494 for model: 1\n",
      "Loss: 0.04625151306390762 for model: 1\n",
      "Loss: 0.04569663479924202 for model: 1\n",
      "Loss: 0.045148685574531555 for model: 1\n",
      "Loss: 0.04460711032152176 for model: 1\n",
      "Loss: 0.04407183453440666 for model: 1\n",
      "Loss: 0.043543945997953415 for model: 1\n",
      "Loss: 0.04302173852920532 for model: 1\n",
      "Loss: 0.042505647987127304 for model: 1\n",
      "Loss: 0.04199579730629921 for model: 1\n",
      "Loss: 0.04149235412478447 for model: 1\n",
      "Loss: 0.04099459946155548 for model: 1\n",
      "Loss: 0.04050309211015701 for model: 1\n",
      "Loss: 0.04001757130026817 for model: 1\n",
      "Loss: 0.03953751549124718 for model: 1\n",
      "Loss: 0.03906356915831566 for model: 1\n",
      "Loss: 0.03859470412135124 for model: 1\n",
      "Loss: 0.03813207522034645 for model: 1\n",
      "Loss: 0.037674617022275925 for model: 1\n",
      "Loss: 0.037222810089588165 for model: 1\n",
      "Loss: 0.03677692264318466 for model: 1\n",
      "Loss: 0.036335691809654236 for model: 1\n",
      "Loss: 0.03590010479092598 for model: 1\n",
      "Loss: 0.035469233989715576 for model: 1\n",
      "Loss: 0.035044118762016296 for model: 1\n",
      "Loss: 0.03462359309196472 for model: 1\n",
      "Loss: 0.0342085063457489 for model: 1\n",
      "Loss: 0.03379814326763153 for model: 1\n",
      "Params tensor([[1.9923, 0.1861]], requires_grad=True) for model: 1\n",
      "\n",
      "Loss: 691.6351318359375 for model: 2\n",
      "Loss: 53.899017333984375 for model: 2\n",
      "Loss: 4.202086925506592 for model: 2\n",
      "Loss: 0.32935792207717896 for model: 2\n",
      "Loss: 0.0275631844997406 for model: 2\n",
      "Loss: 0.004041087348014116 for model: 2\n",
      "Loss: 0.0022034691646695137 for model: 2\n",
      "Loss: 0.0020560340490192175 for model: 2\n",
      "Loss: 0.0020401892252266407 for model: 2\n",
      "Loss: 0.0020345300436019897 for model: 2\n",
      "Loss: 0.002029827795922756 for model: 2\n",
      "Loss: 0.002025080379098654 for model: 2\n",
      "Loss: 0.0020205730106681585 for model: 2\n",
      "Loss: 0.0020158514380455017 for model: 2\n",
      "Loss: 0.0020112458150833845 for model: 2\n",
      "Loss: 0.0020064800046384335 for model: 2\n",
      "Loss: 0.002001781016588211 for model: 2\n",
      "Loss: 0.001997252693399787 for model: 2\n",
      "Loss: 0.0019926251843571663 for model: 2\n",
      "Loss: 0.0019879499450325966 for model: 2\n",
      "Loss: 0.001983497990295291 for model: 2\n",
      "Loss: 0.001978947315365076 for model: 2\n",
      "Loss: 0.0019744141027331352 for model: 2\n",
      "Loss: 0.001969721633940935 for model: 2\n",
      "Loss: 0.0019652550108730793 for model: 2\n",
      "Loss: 0.001960604451596737 for model: 2\n",
      "Loss: 0.0019561832305043936 for model: 2\n",
      "Loss: 0.0019516333704814315 for model: 2\n",
      "Loss: 0.0019472590647637844 for model: 2\n",
      "Loss: 0.001942637376487255 for model: 2\n",
      "Loss: 0.0019381633028388023 for model: 2\n",
      "Loss: 0.0019336414989084005 for model: 2\n",
      "Loss: 0.0019293783698230982 for model: 2\n",
      "Loss: 0.0019248290918767452 for model: 2\n",
      "Loss: 0.0019203536212444305 for model: 2\n",
      "Loss: 0.001915976870805025 for model: 2\n",
      "Loss: 0.0019115714821964502 for model: 2\n",
      "Loss: 0.0019071100978180766 for model: 2\n",
      "Loss: 0.0019026552326977253 for model: 2\n",
      "Loss: 0.001898298622108996 for model: 2\n",
      "Loss: 0.0018939122091978788 for model: 2\n",
      "Loss: 0.001889598905108869 for model: 2\n",
      "Loss: 0.0018852574285119772 for model: 2\n",
      "Loss: 0.0018808860331773758 for model: 2\n",
      "Loss: 0.0018765630666166544 for model: 2\n",
      "Loss: 0.0018722263630479574 for model: 2\n",
      "Loss: 0.001867752056568861 for model: 2\n",
      "Loss: 0.0018636197783052921 for model: 2\n",
      "Loss: 0.001859273761510849 for model: 2\n",
      "Loss: 0.0018549396190792322 for model: 2\n",
      "Params tensor([[2.0070, 0.0698]], requires_grad=True) for model: 2\n",
      "\n",
      "Params for model global model: tensor([[1.6768, 0.4807]], grad_fn=<CloneBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_secure_grads(m = 3, datasets = datasets,workers = workers, iterations = 50  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Additive Secret Sharing\n",
    "\n",
    "While being able to have a trusted third party to perform the aggregation is certainly nice, in an ideal setting we wouldn't have to trust anyone at all. This is where Cryptography can provide an interesting alterantive. \n",
    "\n",
    "Specifically, we're going to be looking at a simple protocol for Secure Multi-Party Computation called Additive Secret Sharing. This protocol will allow multiple parties (of size 3 or more) to aggregate their gradients without the use of a trusted 3rd party to perform the aggregation. In other words, we can add 3 numbers together from 3 different people without anyone ever learning the inputs of any other actors.\n",
    "\n",
    "Let's start by considering the number 5, which we'll put into a varible x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to SHARE the ownership of this number between two people, Alice and Bob. We could split this number into two shares, 2, and 3, and give one to Alice and one to Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "decrypted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that neither Bob nor Alice know the value of x. They only know the value of their own SHARE of x. Thus, the true value of X is hidden (i.e., encrypted). \n",
    "\n",
    "The truly amazing thing, however, is that Alice and Bob can still compute using this value! They can perform arithmetic over the hidden value! Let's say Bob and Alice wanted to multiply this value by 2! If each of them multiplied their respective share by 2, then the hidden number between them is also multiplied! Check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_x_share = 2 * 2\n",
    "alice_x_share = 3 * 2\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "decrypted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This even works for addition between two shared values!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encrypted \"5\"\n",
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "# encrypted \"7\"\n",
    "bob_y_share = 5\n",
    "alice_y_share = 2\n",
    "\n",
    "# encrypted 5 + 7\n",
    "bob_z_share = bob_x_share + bob_y_share\n",
    "alice_z_share = alice_x_share + alice_y_share\n",
    "\n",
    "decrypted_z = bob_z_share + alice_z_share\n",
    "decrypted_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we just added two numbers together while they were still encrypted!!!\n",
    "\n",
    "One small tweak - notice that since all our numbers are positive, it's possible for each share to reveal a little bit of information about the hidden value, namely, it's always greater than the share. Thus, if Bob has a share \"3\" then he knows that the encrypted value is at least 3.\n",
    "\n",
    "This would be quite bad, but can be solved through a simple fix. Decryption happens by summing all the shares together MODULUS some constant. I.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23740629843736686616461"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "\n",
    "Q = 23740629843760239486723\n",
    "\n",
    "bob_x_share = 23552870267 # <- a random number\n",
    "alice_x_share = Q - bob_x_share + x\n",
    "alice_x_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bob_x_share + alice_x_share) % Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, as you can see, both shares are wildly larger than the number being shared, meaning that individual shares no longer leak this inforation. However, all the properties we discussed earlier still hold! (addition, encryption, decryption, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Build Methods for Encrypt, Decrypt, and Add \n",
    "\n",
    "In this project, you must take the lessons we learned in the last section and write general methods for encrypt, decrypt, and add. Store shares for a variable in a tuple like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_share = (2,5,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though normally those shares would be distributed amongst several workers, you can store them in ordered tuples like this for now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this project here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def encrypt(value, n_shares):\n",
    "    \"\"\"\n",
    "    value: int \n",
    "        \n",
    "    n_shares: int \n",
    "        number of shares to split \n",
    "    \"\"\"\n",
    "    \n",
    "    Q = 23740629843760239486723\n",
    "    secure_values = ()\n",
    "    \n",
    "   \n",
    "    for k in range(n_shares):\n",
    "        \n",
    "        if k < n_shares -1:\n",
    "            y = random.randint(-Q,Q)\n",
    "            \n",
    "            secure_values+= (y,)\n",
    "        else:\n",
    "            y = Q \n",
    "            \n",
    "            for i in range(k):\n",
    "                y -=  secure_values[i]\n",
    "            \n",
    "            y = y % Q\n",
    "                \n",
    "            y+=value\n",
    "            \n",
    "                \n",
    "            secure_values+= (y,)\n",
    "        \n",
    "    return secure_values\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "def decrypt(values, Q):\n",
    "    \"\"\"\n",
    "    values: tuple of ints\n",
    "    \n",
    "    Q: int \n",
    "        prime number\n",
    "    \n",
    "    return sum of values module Q\n",
    "    \"\"\"\n",
    "    return sum(values) % Q\n",
    "    \n",
    "def add(values_1, values_2, Q):\n",
    "    assert len(values_1) == len(values_2), \"Must have same length\"\n",
    "    \n",
    "    t = ()\n",
    "    \n",
    "    for i in range(len(values_1)):\n",
    "        t+= ( (values_1[i] + values_2[i]) % Q ,)\n",
    "        \n",
    "    return t\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-15400368266993006068197, -11775402909742148457434, 15311908170804368504784, 21787459108172405796441, 13629521662793987397254, -15435138088330808081638, 22499298251183564261166, 6780452511785644633328, 11176933272122249534673, 22647225819484460939812)\n",
      "(20869319668123403329385, 15091977196243081444462, 6939932336302767932880, 8723022491147967045141, -5793512603149877549856, -22691390493855741069250, -2465736812456750803355, -17678304805536881371525, -23139789255253028199129, 20144482278435059241297)\n",
      "20\n",
      "50\n",
      "(5468951401130397261188, 3316574286500932987028, 22251840507107136437664, 6769851755560133354859, 7836009059644109847398, 9354731105333929822558, 20033561438726813457811, 12842777550009002748526, 11777773860629460822267, 19051078254159280694386)\n"
     ]
    }
   ],
   "source": [
    "values_e  = encrypt(20,10)\n",
    "values_e2  = encrypt(50,10)\n",
    "print(values_e)\n",
    "print(values_e2)\n",
    "\n",
    "value = decrypt(values_e, Q)\n",
    "\n",
    "value2 = decrypt(values_e2, Q)\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(value2)\n",
    "\n",
    "sum_values = add(values_e,values_e2,Q)\n",
    "\n",
    "print(sum_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 1, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2) + (1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Fixed Precision Encoding\n",
    "\n",
    "As you may remember, our goal is to aggregate gradients using this new Secret Sharing technique. However, the protocol we've just explored in the last section uses positive integers. However, our neural network weights are NOT integers. Instead, our weights are decimals (floating point numbers).\n",
    "\n",
    "Not a huge deal! We just need to use a fixed precision encoding, which lets us do computation over decimal numbers using integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE=10\n",
    "PRECISION=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return int((x * (BASE ** PRECISION)) % Q)\n",
    "\n",
    "def decode(x):\n",
    "    return (x if x <= Q/2 else x - Q) / BASE**PRECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encrypt() missing 1 required positional argument: 'n_shares'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-69ca97dddbc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencrypt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencrypt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecrypt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: encrypt() missing 1 required positional argument: 'n_shares'"
     ]
    }
   ],
   "source": [
    "x = encrypt(encode(5.5))\n",
    "y = encrypt(encode(2.3))\n",
    "z = add(x,y)\n",
    "decode(decrypt(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Secret Sharing + Fixed Precision in PySyft\n",
    "\n",
    "While writing things from scratch is certainly educational, PySyft makes a great deal of this much easier for us through its abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-93ff32f147cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0malice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msecure_worker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msecure_worker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bob' is not defined"
     ]
    }
   ],
   "source": [
    "bob = bob.clear_objects()\n",
    "alice = alice.clear_objects()\n",
    "secure_worker = secure_worker.clear_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Sharing Using PySyft\n",
    "\n",
    "We can share using the simple .share() method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-56a37bd6ae41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecure_worker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bob' is not defined"
     ]
    }
   ],
   "source": [
    "x = x.share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-2da03f34bf71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bob' is not defined"
     ]
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as you can see, Bob now has one of the shares of x! Furthermore, we can still call addition in this state, and PySyft will automatically perform the remote execution for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'child'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-c885ac8750dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Instalaciones\\Anaconda3\\envs\\privateai\\lib\\site-packages\\syft\\frameworks\\torch\\tensors\\interpreters\\native.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, inplace, *args, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m#                     return self\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;31m# Clean the wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'child'"
     ]
    }
   ],
   "source": [
    "y.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Precision using PySyft\n",
    "\n",
    "We can also convert a tensor to fixed precision using .fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.fix_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 200, 300])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.child.child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.float_prec()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Fixed Precision\n",
    "\n",
    "And of course, we can combine the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob,alice,secure_worker = (sy.VirtualWorker(hook, id = x ) for x in [\"bob\",\"alice\",\"secure_worker\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.fix_prec().share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Wrapper)>FixedPrecisionTensor>(Wrapper)>[AdditiveSharingTensor]\n",
       " \t-> (Wrapper)>[PointerTensor | me:71611505858 -> bob:84124392813]\n",
       " \t-> (Wrapper)>[PointerTensor | me:22254103362 -> alice:12350534841]\n",
       " \t-> (Wrapper)>[PointerTensor | me:33595740445 -> secure_worker:53031209616]\n",
       " \t*crypto provider: me*,\n",
       " (Wrapper)>FixedPrecisionTensor>(Wrapper)>[AdditiveSharingTensor]\n",
       " \t-> (Wrapper)>[PointerTensor | me:44513523097 -> bob:90324360530]\n",
       " \t-> (Wrapper)>[PointerTensor | me:64184315972 -> alice:16446497511]\n",
       " \t-> (Wrapper)>[PointerTensor | me:44540902778 -> secure_worker:4763932519]\n",
       " \t*crypto provider: me*)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get().float_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to make the point that people can see the model averages in the clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<VirtualWorker id:w1 #objects:0>,\n",
       " <VirtualWorker id:w2 #objects:0>,\n",
       " <VirtualWorker id:w3 #objects:0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.clear_objects(),w2.clear_objects(),w3.clear_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {}, {})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1._objects,w2._objects,w3._objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2,w3 = (sy.VirtualWorker(hook, id = \"w\" + str(i)) for i in range(1,4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = th.tensor([1,1,1,1]), th.tensor([5,5,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.share(w1,w2,w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.share(w1,w2,w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:55259610816 -> w1:92321648997]\n",
       "\t-> (Wrapper)>[PointerTensor | me:79959464640 -> w2:30549673585]\n",
       "\t-> (Wrapper)>[PointerTensor | me:58414339208 -> w3:10270487504]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Federated Learning with Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 13:03:09.509400 17824 secure_random.py:22] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.14.0). Fix this by compiling custom ops.\n",
      "W0710 13:03:09.563257 17824 deprecation_wrapper.py:119] From F:\\Instalaciones\\Anaconda3\\envs\\privateai\\lib\\site-packages\\tf_encrypted\\session.py:28: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch  import nn,optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = th.tensor([[1.0,1.0],[3,1.0],[1,1.0],[0,1.0],[23.0,1.0],[6,1.0],[7,1.0],[15,1.0],[9.0,1.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "target = th.tensor([[2.],[6.0], [2.0], [0],[46.0],[12.0],[14.0],[30.0],[18.0]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=  3 #number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers  =[sy.VirtualWorker(hook, id = \"w\"+ str(i)) for i in range(m)]\n",
    "chunk_size = data.shape[0]//m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets[0]\n",
      "tensor([[1., 1.],\n",
      "        [3., 1.],\n",
      "        [1., 1.]], grad_fn=<SliceBackward>)\n",
      "tensor([[2.],\n",
      "        [6.],\n",
      "        [2.]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "#make a mini-dataset, one per worker\n",
    "datasets = [ [\n",
    "              data[j*chunk_size : (j+1) * chunk_size  ] ,\n",
    "              target[j*chunk_size : (j+1) * chunk_size] \n",
    "             ]\n",
    "            \n",
    "             for  j in range(m)\n",
    "           ]\n",
    "\n",
    "print(\"Datasets[0]\", *datasets[0], sep= \"\\n\")\n",
    "\n",
    "#send datasets to workers\n",
    "for k in range(m):\n",
    "    \n",
    "    datasets[k][0] = datasets[k][0].send(workers[k])\n",
    "    \n",
    "    datasets[k][1] = datasets[k][1].send(workers[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def train_additive_secret_sharing(m,datasets,workers, iterations=20):\n",
    "    \"\"\"\n",
    "    iterations: int\n",
    "    \n",
    "    m: int\n",
    "        number of models/workers\n",
    "        \n",
    "    datasets: list of lists of tensors\n",
    "        [ [data_1,targets_1],[data_2, targets_2], ...     ]\n",
    "    \n",
    "    workers: list of VirtualWorkers\n",
    "    \n",
    "    iterations: int \n",
    "        number of SGD steps to do.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a model for each worker \n",
    "    models = [ nn.Linear(2,1) for _ in range(m)]\n",
    "    \n",
    "    \n",
    "    # Global model\n",
    "    global_model = nn.Linear(2,1)\n",
    "    \n",
    "    # Create optims \n",
    "    optims = []\n",
    "\n",
    "    for i,t in enumerate(datasets):\n",
    "        \n",
    "        _data,_target  = t[0],t[1]\n",
    "        \n",
    "        \n",
    "        # Data preparation\n",
    "        \n",
    "        # send model to the data\n",
    "        models[i] = models[i].send(_data.location)\n",
    "        \n",
    "        #create optimizer for model i \n",
    "        #learning rate >= 0.1, makess trainig diverge on most models with little data.\n",
    "        optim_i = optim.SGD(params=models[i].parameters(), lr=0.001)\n",
    "        optims.extend( [optim_i] )\n",
    "        \n",
    "        for _ in range(iterations):            \n",
    "            # do normal training\n",
    "            optims[i].zero_grad()\n",
    "            pred = models[i](_data)\n",
    "            loss = ((pred - _target)**2).sum()\n",
    "            loss.backward()\n",
    "            \n",
    "            optims[i].step()\n",
    "\n",
    "            \n",
    "            print(\"Loss: {} for model: {}\".format( loss.clone().get().item(), i ))\n",
    "        \n",
    "        print(\"Params {} for model: {}\".format(models[i].weight.clone().get() ,i))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Average models using additive secret sharing.\n",
    "   \n",
    "    w,b = None,None\n",
    "    \n",
    "    for k,model in enumerate(models):\n",
    "        \n",
    "        # Share model params between workers\n",
    "        if k == 0:\n",
    "            w = model.weight.data.clone().get()\n",
    "            w = w.share(*workers)\n",
    "            \n",
    "            b = model.bias.data.clone().get()\n",
    "            b = b.share(*workers)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            w+= model.weight.data.clone().get().share(*workers)\n",
    "            b+=  model.bias.data.clone().get().share(*workers)\n",
    "    \n",
    "    \n",
    "    with th.no_grad():\n",
    "    \n",
    "        global_model.weight.set_(  w.get().float()\n",
    "                                  )\n",
    "        global_model.bias.set_(    b.get().float() \n",
    "                                  )\n",
    "    \n",
    "    print(\"Params for model global model: {}\".format(global_model.weight.clone()))\n",
    "\n",
    "        \n",
    "    return global_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 41.730979919433594 for model: 0\n",
      "Loss: 39.138465881347656 for model: 0\n",
      "Loss: 36.70909881591797 for model: 0\n",
      "Loss: 34.43257522583008 for model: 0\n",
      "Loss: 32.29927062988281 for model: 0\n",
      "Loss: 30.30017852783203 for model: 0\n",
      "Loss: 28.42683982849121 for model: 0\n",
      "Loss: 26.67133903503418 for model: 0\n",
      "Loss: 25.02625274658203 for model: 0\n",
      "Loss: 23.484628677368164 for model: 0\n",
      "Loss: 22.039953231811523 for model: 0\n",
      "Loss: 20.686120986938477 for model: 0\n",
      "Loss: 19.417409896850586 for model: 0\n",
      "Loss: 18.228464126586914 for model: 0\n",
      "Loss: 17.114255905151367 for model: 0\n",
      "Loss: 16.070079803466797 for model: 0\n",
      "Loss: 15.091530799865723 for model: 0\n",
      "Loss: 14.17447280883789 for model: 0\n",
      "Loss: 13.315030097961426 for model: 0\n",
      "Loss: 12.509580612182617 for model: 0\n",
      "Params tensor([[0.8863, 0.0022]], requires_grad=True) for model: 0\n",
      "\n",
      "Loss: 2675.000244140625 for model: 1\n",
      "Loss: 49.47021484375 for model: 1\n",
      "Loss: 0.918511152267456 for model: 1\n",
      "Loss: 0.02064458839595318 for model: 1\n",
      "Loss: 0.003995848353952169 for model: 1\n",
      "Loss: 0.0036447569727897644 for model: 1\n",
      "Loss: 0.003595402929931879 for model: 1\n",
      "Loss: 0.00355220353230834 for model: 1\n",
      "Loss: 0.0035095405764877796 for model: 1\n",
      "Loss: 0.003467450151219964 for model: 1\n",
      "Loss: 0.0034259085077792406 for model: 1\n",
      "Loss: 0.0033848434686660767 for model: 1\n",
      "Loss: 0.003344249678775668 for model: 1\n",
      "Loss: 0.0033041839487850666 for model: 1\n",
      "Loss: 0.0032644513994455338 for model: 1\n",
      "Loss: 0.0032253768295049667 for model: 1\n",
      "Loss: 0.003186736721545458 for model: 1\n",
      "Loss: 0.0031484896317124367 for model: 1\n",
      "Loss: 0.0031106751412153244 for model: 1\n",
      "Loss: 0.003073363797739148 for model: 1\n",
      "Params tensor([[1.9977, 0.0201]], requires_grad=True) for model: 1\n",
      "\n",
      "Loss: 1178.4993896484375 for model: 2\n",
      "Loss: 92.43693542480469 for model: 2\n",
      "Loss: 7.801904678344727 for model: 2\n",
      "Loss: 1.2051525115966797 for model: 2\n",
      "Loss: 0.6897159814834595 for model: 2\n",
      "Loss: 0.6481743454933167 for model: 2\n",
      "Loss: 0.6435640454292297 for model: 2\n",
      "Loss: 0.6418392658233643 for model: 2\n",
      "Loss: 0.6403390169143677 for model: 2\n",
      "Loss: 0.6388593912124634 for model: 2\n",
      "Loss: 0.6373825073242188 for model: 2\n",
      "Loss: 0.635914146900177 for model: 2\n",
      "Loss: 0.6344475150108337 for model: 2\n",
      "Loss: 0.6329841613769531 for model: 2\n",
      "Loss: 0.6315234899520874 for model: 2\n",
      "Loss: 0.6300680041313171 for model: 2\n",
      "Loss: 0.6286136507987976 for model: 2\n",
      "Loss: 0.627163290977478 for model: 2\n",
      "Loss: 0.6257167458534241 for model: 2\n",
      "Loss: 0.6242731809616089 for model: 2\n",
      "Params tensor([[1.8725, 0.7877]], requires_grad=True) for model: 2\n",
      "\n",
      "Params for model global model: tensor([[2., 0.]], grad_fn=<CloneBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_additive_secret_sharing(m = 3,datasets = datasets,workers = workers, iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privateai",
   "language": "python",
   "name": "privateai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
