{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Securing Federated Learning\n",
    "\n",
    "- Lesson 1: Trusted Aggregator\n",
    "- Lesson 2: Intro to Additive Secret Sharing\n",
    "- Lesson 3: Intro to Fixed Precision Encoding\n",
    "- Lesson 4: Secret Sharing + Fixed Precision in PySyft\n",
    "- Final Project: Federated Learning wtih Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Federated Learning with a Trusted Aggregator\n",
    "\n",
    "In the last section, we learned how to train a model on a distributed dataset using Federated Learning. In particular, the last project aggregated gradients directly from one data owner to another. \n",
    "\n",
    "However, while in some cases it could be ideal to do this, what would be even better is to be able to choose a neutral third party to perform the aggregation.\n",
    "\n",
    "As it turns out, we can use the same tools we used previously to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Federated Learning with a Trusted Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this project here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0708 14:08:34.462585  4204 secure_random.py:22] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.14.0). Fix this by compiling custom ops.\n",
      "W0708 14:08:34.633329  4204 deprecation_wrapper.py:119] From F:\\Instalaciones\\Anaconda3\\envs\\privateai\\lib\\site-packages\\tf_encrypted\\session.py:28: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch as th\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = th.tensor([[1.0,1.0],[3,1.0],[1,1.0],[0,1.0],[23.0,1.0],[6,1.0],[7,1.0],[15,1.0],[9.0,1.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "target = th.tensor([[2.],[6.0], [2.0], [0],[46.0],[12.0],[14.0],[30.0],[18.0]], requires_grad=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3 #number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers  =[sy.VirtualWorker(hook, id = \"w\"+ str(i)) for i in range(m)]\n",
    "chunk_size = data.shape[0]//m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets[0]\n",
      "tensor([[1., 1.],\n",
      "        [3., 1.],\n",
      "        [1., 1.]], grad_fn=<SliceBackward>)\n",
      "tensor([[2.],\n",
      "        [6.],\n",
      "        [2.]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "#make a mini-dataset, one per worker\n",
    "datasets = [ [\n",
    "              data[j*chunk_size : (j+1) * chunk_size  ] ,\n",
    "              target[j*chunk_size : (j+1) * chunk_size] \n",
    "             ]\n",
    "            \n",
    "             for  j in range(m)\n",
    "           ]\n",
    "\n",
    "print(\"Datasets[0]\", *datasets[0], sep= \"\\n\")\n",
    "\n",
    "#send datasets to workers\n",
    "for k in range(m):\n",
    "    \n",
    "    datasets[k][0] = datasets[k][0].send(workers[k])\n",
    "    \n",
    "    datasets[k][1] = datasets[k][1].send(workers[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "def train_secure_grads(m,datasets,workers, iterations=20):\n",
    "    \"\"\"\n",
    "    iterations: int\n",
    "    \n",
    "    m: int\n",
    "        number of models/workers\n",
    "        \n",
    "    datasets: list of lists of tensors\n",
    "        [ [data_1,targets_1],[data_2, targets_2], ...     ]\n",
    "    \n",
    "    workers: list of VirtualWorkers\n",
    "    \n",
    "    iterations: int \n",
    "        number of SGD steps to do.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a model for each worker \n",
    "    models = [ nn.Linear(2,1) for _ in range(m)]\n",
    "    \n",
    "    \n",
    "    # Global model\n",
    "    global_model = nn.Linear(2,1)\n",
    "    \n",
    "    # Create optims \n",
    "    optims = []\n",
    "\n",
    "    for i,t in enumerate(datasets):\n",
    "        \n",
    "        _data,_target  = t[0],t[1]\n",
    "        \n",
    "        \n",
    "        # Data preparation\n",
    "        \n",
    "        # send model to the data\n",
    "        models[i] = models[i].send(_data.location)\n",
    "        \n",
    "        #create optimizer for model i \n",
    "        #learning rate >= 0.1, makess trainig diverge on most models with little data.\n",
    "        optim_i = optim.SGD(params=models[i].parameters(), lr=0.001)\n",
    "        optims.extend( [optim_i] )\n",
    "        \n",
    "        for _ in range(iterations):            \n",
    "            # do normal training\n",
    "            optims[i].zero_grad()\n",
    "            pred = models[i](_data)\n",
    "            loss = ((pred - _target)**2).sum()\n",
    "            loss.backward()\n",
    "            \n",
    "            optims[i].step()\n",
    "\n",
    "            \n",
    "            print(\"Loss: {} for model: {}\".format( loss.clone().get().item(), i ))\n",
    "        \n",
    "        print(\"Params {} for model: {}\".format(models[i].weight.clone().get() ,i))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Average models on selected worker\n",
    "    \n",
    "    #shuffle models. Select first to aggregate gradients\n",
    "    shuffle(models)\n",
    "    \n",
    "    #Trusted agregator\n",
    "    # Selected worker to aggregate gradients.\n",
    "    sel_model = models[0]\n",
    "    \n",
    "    #move_model = lambda x: x.move(sel_worker)\n",
    "\n",
    "    w,b = sel_model.weight.data ,sel_model.bias.data\n",
    "    \n",
    "    for model in models[1:]:\n",
    "        # Move model to selected worker\n",
    "        model.move(sel_model.location)\n",
    "        \n",
    "        w+= model.weight.data\n",
    "        b+=  model.bias.data\n",
    "    \n",
    "    w = w/m\n",
    "    b = b/m\n",
    "    \n",
    "    with th.no_grad():\n",
    "    \n",
    "        global_model.weight.set_(  w.get() \n",
    "                                  )\n",
    "        global_model.bias.set_(    b.get() \n",
    "                                  )\n",
    "    \n",
    "    print(\"Params for model global model: {}\".format(global_model.weight.clone()))\n",
    "\n",
    "        \n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 50.22532272338867 for model: 0\n",
      "Loss: 47.13780212402344 for model: 0\n",
      "Loss: 44.24441909790039 for model: 0\n",
      "Loss: 41.5329475402832 for model: 0\n",
      "Loss: 38.99193572998047 for model: 0\n",
      "Loss: 36.61064910888672 for model: 0\n",
      "Loss: 34.3790283203125 for model: 0\n",
      "Loss: 32.28765106201172 for model: 0\n",
      "Loss: 30.327686309814453 for model: 0\n",
      "Loss: 28.490863800048828 for model: 0\n",
      "Loss: 26.7694149017334 for model: 0\n",
      "Loss: 25.156097412109375 for model: 0\n",
      "Loss: 23.64408302307129 for model: 0\n",
      "Loss: 22.226999282836914 for model: 0\n",
      "Loss: 20.89887046813965 for model: 0\n",
      "Loss: 19.654098510742188 for model: 0\n",
      "Loss: 18.4874324798584 for model: 0\n",
      "Loss: 17.393951416015625 for model: 0\n",
      "Loss: 16.369054794311523 for model: 0\n",
      "Loss: 15.408417701721191 for model: 0\n",
      "Loss: 14.507997512817383 for model: 0\n",
      "Loss: 13.6640043258667 for model: 0\n",
      "Loss: 12.872881889343262 for model: 0\n",
      "Loss: 12.131303787231445 for model: 0\n",
      "Loss: 11.436153411865234 for model: 0\n",
      "Loss: 10.784505844116211 for model: 0\n",
      "Loss: 10.173619270324707 for model: 0\n",
      "Loss: 9.600933074951172 for model: 0\n",
      "Loss: 9.06403923034668 for model: 0\n",
      "Loss: 8.560687065124512 for model: 0\n",
      "Loss: 8.088763236999512 for model: 0\n",
      "Loss: 7.646292209625244 for model: 0\n",
      "Loss: 7.231420040130615 for model: 0\n",
      "Loss: 6.842408180236816 for model: 0\n",
      "Loss: 6.477625846862793 for model: 0\n",
      "Loss: 6.135554790496826 for model: 0\n",
      "Loss: 5.814763069152832 for model: 0\n",
      "Loss: 5.51391077041626 for model: 0\n",
      "Loss: 5.231742858886719 for model: 0\n",
      "Loss: 4.967086315155029 for model: 0\n",
      "Loss: 4.718835353851318 for model: 0\n",
      "Loss: 4.485963344573975 for model: 0\n",
      "Loss: 4.267497539520264 for model: 0\n",
      "Loss: 4.062535762786865 for model: 0\n",
      "Loss: 3.8702235221862793 for model: 0\n",
      "Loss: 3.689772367477417 for model: 0\n",
      "Loss: 3.5204296112060547 for model: 0\n",
      "Loss: 3.361501693725586 for model: 0\n",
      "Loss: 3.212329864501953 for model: 0\n",
      "Loss: 3.0723037719726562 for model: 0\n",
      "Params tensor([[1.1282, 0.8322]], requires_grad=True) for model: 0\n",
      "\n",
      "Loss: 2152.29931640625 for model: 1\n",
      "Loss: 39.852176666259766 for model: 1\n",
      "Loss: 0.7877796292304993 for model: 1\n",
      "Loss: 0.06478534638881683 for model: 1\n",
      "Loss: 0.050812385976314545 for model: 1\n",
      "Loss: 0.04995884746313095 for model: 1\n",
      "Loss: 0.049355536699295044 for model: 1\n",
      "Loss: 0.048763588070869446 for model: 1\n",
      "Loss: 0.048178963363170624 for model: 1\n",
      "Loss: 0.04760138690471649 for model: 1\n",
      "Loss: 0.047030262649059296 for model: 1\n",
      "Loss: 0.046466268599033356 for model: 1\n",
      "Loss: 0.04590925946831703 for model: 1\n",
      "Loss: 0.04535825923085213 for model: 1\n",
      "Loss: 0.04481462761759758 for model: 1\n",
      "Loss: 0.04427727311849594 for model: 1\n",
      "Loss: 0.043745964765548706 for model: 1\n",
      "Loss: 0.04322154447436333 for model: 1\n",
      "Loss: 0.04270322993397713 for model: 1\n",
      "Loss: 0.042190950363874435 for model: 1\n",
      "Loss: 0.04168536886572838 for model: 1\n",
      "Loss: 0.041185230016708374 for model: 1\n",
      "Loss: 0.04069138690829277 for model: 1\n",
      "Loss: 0.04020312428474426 for model: 1\n",
      "Loss: 0.039721257984638214 for model: 1\n",
      "Loss: 0.03924502432346344 for model: 1\n",
      "Loss: 0.038774389773607254 for model: 1\n",
      "Loss: 0.038309283554553986 for model: 1\n",
      "Loss: 0.03785010799765587 for model: 1\n",
      "Loss: 0.03739587962627411 for model: 1\n",
      "Loss: 0.03694746643304825 for model: 1\n",
      "Loss: 0.03650432080030441 for model: 1\n",
      "Loss: 0.036066874861717224 for model: 1\n",
      "Loss: 0.03563455119729042 for model: 1\n",
      "Loss: 0.03520694747567177 for model: 1\n",
      "Loss: 0.034784622490406036 for model: 1\n",
      "Loss: 0.03436773642897606 for model: 1\n",
      "Loss: 0.03395514562726021 for model: 1\n",
      "Loss: 0.03354811668395996 for model: 1\n",
      "Loss: 0.033145882189273834 for model: 1\n",
      "Loss: 0.032748475670814514 for model: 1\n",
      "Loss: 0.03235577791929245 for model: 1\n",
      "Loss: 0.03196759149432182 for model: 1\n",
      "Loss: 0.0315842479467392 for model: 1\n",
      "Loss: 0.031205467879772186 for model: 1\n",
      "Loss: 0.03083142265677452 for model: 1\n",
      "Loss: 0.03046126291155815 for model: 1\n",
      "Loss: 0.03009614907205105 for model: 1\n",
      "Loss: 0.02973521128296852 for model: 1\n",
      "Loss: 0.029378607869148254 for model: 1\n",
      "Params tensor([[ 1.9928, -0.0436]], requires_grad=True) for model: 1\n",
      "\n",
      "Loss: 1912.42724609375 for model: 2\n",
      "Loss: 149.40899658203125 for model: 2\n",
      "Loss: 12.021356582641602 for model: 2\n",
      "Loss: 1.3142569065093994 for model: 2\n",
      "Loss: 0.4790123701095581 for model: 2\n",
      "Loss: 0.4130542278289795 for model: 2\n",
      "Loss: 0.4070490598678589 for model: 2\n",
      "Loss: 0.40571534633636475 for model: 2\n",
      "Loss: 0.4047490060329437 for model: 2\n",
      "Loss: 0.4038129150867462 for model: 2\n",
      "Loss: 0.4028799831867218 for model: 2\n",
      "Loss: 0.4019523561000824 for model: 2\n",
      "Loss: 0.4010251760482788 for model: 2\n",
      "Loss: 0.4000994563102722 for model: 2\n",
      "Loss: 0.3991771340370178 for model: 2\n",
      "Loss: 0.39825403690338135 for model: 2\n",
      "Loss: 0.39733606576919556 for model: 2\n",
      "Loss: 0.3964192867279053 for model: 2\n",
      "Loss: 0.3955038785934448 for model: 2\n",
      "Loss: 0.3945913314819336 for model: 2\n",
      "Loss: 0.39368245005607605 for model: 2\n",
      "Loss: 0.3927738666534424 for model: 2\n",
      "Loss: 0.3918684720993042 for model: 2\n",
      "Loss: 0.3909640610218048 for model: 2\n",
      "Loss: 0.3900611996650696 for model: 2\n",
      "Loss: 0.389161616563797 for model: 2\n",
      "Loss: 0.3882635533809662 for model: 2\n",
      "Loss: 0.38736772537231445 for model: 2\n",
      "Loss: 0.3864752948284149 for model: 2\n",
      "Loss: 0.385583758354187 for model: 2\n",
      "Loss: 0.3846946060657501 for model: 2\n",
      "Loss: 0.38380515575408936 for model: 2\n",
      "Loss: 0.38291987776756287 for model: 2\n",
      "Loss: 0.3820359706878662 for model: 2\n",
      "Loss: 0.381154865026474 for model: 2\n",
      "Loss: 0.3802756667137146 for model: 2\n",
      "Loss: 0.3793983459472656 for model: 2\n",
      "Loss: 0.3785233497619629 for model: 2\n",
      "Loss: 0.3776493966579437 for model: 2\n",
      "Loss: 0.37677812576293945 for model: 2\n",
      "Loss: 0.37590959668159485 for model: 2\n",
      "Loss: 0.37504294514656067 for model: 2\n",
      "Loss: 0.37417736649513245 for model: 2\n",
      "Loss: 0.37331444025039673 for model: 2\n",
      "Loss: 0.37245118618011475 for model: 2\n",
      "Loss: 0.37159469723701477 for model: 2\n",
      "Loss: 0.37073609232902527 for model: 2\n",
      "Loss: 0.36988064646720886 for model: 2\n",
      "Loss: 0.3690274953842163 for model: 2\n",
      "Loss: 0.3681752383708954 for model: 2\n",
      "Params tensor([[1.9021, 0.7095]], requires_grad=True) for model: 2\n",
      "\n",
      "Params for model global model: tensor([[1.6744, 0.4993]], grad_fn=<CloneBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_secure_grads(m = 3, datasets = datasets,workers = workers, iterations = 50  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Additive Secret Sharing\n",
    "\n",
    "While being able to have a trusted third party to perform the aggregation is certainly nice, in an ideal setting we wouldn't have to trust anyone at all. This is where Cryptography can provide an interesting alterantive. \n",
    "\n",
    "Specifically, we're going to be looking at a simple protocol for Secure Multi-Party Computation called Additive Secret Sharing. This protocol will allow multiple parties (of size 3 or more) to aggregate their gradients without the use of a trusted 3rd party to perform the aggregation. In other words, we can add 3 numbers together from 3 different people without anyone ever learning the inputs of any other actors.\n",
    "\n",
    "Let's start by considering the number 5, which we'll put into a varible x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to SHARE the ownership of this number between two people, Alice and Bob. We could split this number into two shares, 2, and 3, and give one to Alice and one to Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "decrypted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that neither Bob nor Alice know the value of x. They only know the value of their own SHARE of x. Thus, the true value of X is hidden (i.e., encrypted). \n",
    "\n",
    "The truly amazing thing, however, is that Alice and Bob can still compute using this value! They can perform arithmetic over the hidden value! Let's say Bob and Alice wanted to multiply this value by 2! If each of them multiplied their respective share by 2, then the hidden number between them is also multiplied! Check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob_x_share = 2 * 2\n",
    "alice_x_share = 3 * 2\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "decrypted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This even works for addition between two shared values!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encrypted \"5\"\n",
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "# encrypted \"7\"\n",
    "bob_y_share = 5\n",
    "alice_y_share = 2\n",
    "\n",
    "# encrypted 5 + 7\n",
    "bob_z_share = bob_x_share + bob_y_share\n",
    "alice_z_share = alice_x_share + alice_y_share\n",
    "\n",
    "decrypted_z = bob_z_share + alice_z_share\n",
    "decrypted_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we just added two numbers together while they were still encrypted!!!\n",
    "\n",
    "One small tweak - notice that since all our numbers are positive, it's possible for each share to reveal a little bit of information about the hidden value, namely, it's always greater than the share. Thus, if Bob has a share \"3\" then he knows that the encrypted value is at least 3.\n",
    "\n",
    "This would be quite bad, but can be solved through a simple fix. Decryption happens by summing all the shares together MODULUS some constant. I.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23740629843736686616461"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "\n",
    "Q = 23740629843760239486723\n",
    "\n",
    "bob_x_share = 23552870267 # <- a random number\n",
    "alice_x_share = Q - bob_x_share + x\n",
    "alice_x_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bob_x_share + alice_x_share) % Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, as you can see, both shares are wildly larger than the number being shared, meaning that individual shares no longer leak this inforation. However, all the properties we discussed earlier still hold! (addition, encryption, decryption, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Build Methods for Encrypt, Decrypt, and Add \n",
    "\n",
    "In this project, you must take the lessons we learned in the last section and write general methods for encrypt, decrypt, and add. Store shares for a variable in a tuple like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_share = (2,5,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though normally those shares would be distributed amongst several workers, you can store them in ordered tuples like this for now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this project here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encrypt(value, n_shares):\n",
    "    \"\"\"\n",
    "    value: int \n",
    "        \n",
    "    n_shares: int \n",
    "        number of shares to split \n",
    "    \"\"\"\n",
    "    \n",
    "    Q = 23740629843760239486723\n",
    "    secure_values = ()\n",
    "    \n",
    "   \n",
    "    for k in range(n_shares):\n",
    "        \n",
    "        if k < n_shares -1:\n",
    "            y = random.randint(-Q,Q)\n",
    "            \n",
    "            secure_values+= (y,)\n",
    "        else:\n",
    "            y = Q \n",
    "            \n",
    "            for i in range(k):\n",
    "                y -=  secure_values[i]\n",
    "            \n",
    "            y = y \n",
    "                \n",
    "            y+=value\n",
    "            \n",
    "                \n",
    "            secure_values+= (y%Q,)\n",
    "        \n",
    "    return secure_values\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "def decrypt(values, Q):\n",
    "    \"\"\"\n",
    "    values: tuple of ints\n",
    "    \n",
    "    Q: int \n",
    "        prime number\n",
    "    \n",
    "    return sum of values module Q\n",
    "    \"\"\"\n",
    "    return sum(values) % Q\n",
    "    \n",
    "def add(values_1, values_2):\n",
    "    assert len(values_1) == len(values_2), \"Must have same length\"\n",
    "    \n",
    "    t = ()\n",
    "    \n",
    "    for i in range(len(values_1)):\n",
    "        t+= (values_1[i] + values_2[i],)\n",
    "        \n",
    "    return t\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1528271370777591110777, 5779821831933499614875, 8295867601780705412258, 2154123704320032448861, -12240454052228198716105, 17621004172772501258807, 434758398161374541654, 15034450132530383436056, -6093923371985386451816, 14967339899457976318099)\n",
      "(-15860756206237882735303, -19892743624202390476181, -20308909361260895918134, 14672650148976093384261, 11335086358643071617029, 1686884817763748227564, -15717140250040993001689, 6409710382268790464901, 6175103156456623243899, 7759484733873595706980)\n",
      "20\n",
      "50\n",
      "(-14332484835460291624526, -14112921792268890861306, -12013041759480190505876, 16826773853296125833122, -905367693585127099076, 19307888990536249486371, -15282381851879618460035, 21444160514799173900957, 81179784471236792083, 22726824633331572025079)\n"
     ]
    }
   ],
   "source": [
    "values_e  = encrypt(20,10)\n",
    "values_e2  = encrypt(50,10)\n",
    "print(values_e)\n",
    "print(values_e2)\n",
    "\n",
    "value = decrypt(values_e, Q)\n",
    "\n",
    "value2 = decrypt(values_e2, Q)\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(value2)\n",
    "\n",
    "sum_values = add(values_e,values_e2)\n",
    "\n",
    "print(sum_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 1, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2) + (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Fixed Precision Encoding\n",
    "\n",
    "As you may remember, our goal is to aggregate gradients using this new Secret Sharing technique. However, the protocol we've just explored in the last section uses positive integers. However, our neural network weights are NOT integers. Instead, our weights are decimals (floating point numbers).\n",
    "\n",
    "Not a huge deal! We just need to use a fixed precision encoding, which lets us do computation over decimal numbers using integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE=10\n",
    "PRECISION=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return int((x * (BASE ** PRECISION)) % Q)\n",
    "\n",
    "def decode(x):\n",
    "    return (x if x <= Q/2 else x - Q) / BASE**PRECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encrypt(encode(5.5))\n",
    "y = encrypt(encode(2.3))\n",
    "z = add(x,y)\n",
    "decode(decrypt(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Secret Sharing + Fixed Precision in PySyft\n",
    "\n",
    "While writing things from scratch is certainly educational, PySyft makes a great deal of this much easier for us through its abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = bob.clear_objects()\n",
    "alice = alice.clear_objects()\n",
    "secure_worker = secure_worker.clear_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Sharing Using PySyft\n",
    "\n",
    "We can share using the simple .share() method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35498656553: tensor([  10235770278698899, 1401398179551373756, 2277280072169145491,\n",
       "          636965538565031298,  913795591610271305])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as you can see, Bob now has one of the shares of x! Furthermore, we can still call addition in this state, and PySyft will automatically perform the remote execution for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:23637986557 -> bob:30254176063]\n",
       "\t-> (Wrapper)>[PointerTensor | me:18229131498 -> alice:75856222543]\n",
       "\t-> (Wrapper)>[PointerTensor | me:34301722959 -> secure_worker:75419815101]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Precision using PySyft\n",
    "\n",
    "We can also convert a tensor to fixed precision using .fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.fix_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 200, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.child.child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.float_prec()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Fixed Precision\n",
    "\n",
    "And of course, we can combine the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.tensor([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.fix_prec().share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get().float_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to make the point that people can see the model averages in the clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Federated Learning with Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privateai",
   "language": "python",
   "name": "privateai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
